{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b18c481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_17740\\3402801710.py:10: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_surprise = pd.read_csv('US_economic_releases_events.csv')\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_17740\\3402801710.py:20: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_surprise.replace(\"--\", pd.NA, inplace=True)\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_17740\\3402801710.py:108: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_combined['First Post Surprise'].iloc[0] = False\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_17740\\3402801710.py:108: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_combined['First Post Surprise'].iloc[0] = False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_es</th>\n",
       "      <th>Time_es</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Date_surprise</th>\n",
       "      <th>Period</th>\n",
       "      <th>Event</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>...</th>\n",
       "      <th>Day</th>\n",
       "      <th>Surv(M)</th>\n",
       "      <th># Ests.</th>\n",
       "      <th>Std Dev</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>Flag</th>\n",
       "      <th>_merge</th>\n",
       "      <th>Surprise Occurred</th>\n",
       "      <th>First Post Surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>09/10/1997</td>\n",
       "      <td>00:01:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1997-09-10 00:01:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>09/10/1997</td>\n",
       "      <td>00:02:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1997-09-10 00:02:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>09/10/1997</td>\n",
       "      <td>00:03:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1997-09-10 00:03:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>09/10/1997</td>\n",
       "      <td>00:04:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1997-09-10 00:04:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>09/10/1997</td>\n",
       "      <td>00:05:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1997-09-10 00:05:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9695761</th>\n",
       "      <td>12/19/2024</td>\n",
       "      <td>15:56:00</td>\n",
       "      <td>5941.75</td>\n",
       "      <td>5941.75</td>\n",
       "      <td>318.0</td>\n",
       "      <td>2024-12-19 15:56:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9695762</th>\n",
       "      <td>12/19/2024</td>\n",
       "      <td>15:57:00</td>\n",
       "      <td>5941.75</td>\n",
       "      <td>5941.50</td>\n",
       "      <td>386.0</td>\n",
       "      <td>2024-12-19 15:57:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9695763</th>\n",
       "      <td>12/19/2024</td>\n",
       "      <td>15:58:00</td>\n",
       "      <td>5941.50</td>\n",
       "      <td>5941.00</td>\n",
       "      <td>484.0</td>\n",
       "      <td>2024-12-19 15:58:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9695764</th>\n",
       "      <td>12/19/2024</td>\n",
       "      <td>15:59:00</td>\n",
       "      <td>5940.75</td>\n",
       "      <td>5941.00</td>\n",
       "      <td>6462.0</td>\n",
       "      <td>2024-12-19 15:59:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9695765</th>\n",
       "      <td>12/19/2024</td>\n",
       "      <td>16:00:00</td>\n",
       "      <td>5941.00</td>\n",
       "      <td>5941.25</td>\n",
       "      <td>8864.0</td>\n",
       "      <td>2024-12-19 16:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9695665 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date_es   Time_es     Open    Close  Volume            DateTime  \\\n",
       "2        09/10/1997  00:01:00     0.00     0.00     0.0 1997-09-10 00:01:00   \n",
       "3        09/10/1997  00:02:00     0.00     0.00     0.0 1997-09-10 00:02:00   \n",
       "4        09/10/1997  00:03:00     0.00     0.00     0.0 1997-09-10 00:03:00   \n",
       "5        09/10/1997  00:04:00     0.00     0.00     0.0 1997-09-10 00:04:00   \n",
       "6        09/10/1997  00:05:00     0.00     0.00     0.0 1997-09-10 00:05:00   \n",
       "...             ...       ...      ...      ...     ...                 ...   \n",
       "9695761  12/19/2024  15:56:00  5941.75  5941.75   318.0 2024-12-19 15:56:00   \n",
       "9695762  12/19/2024  15:57:00  5941.75  5941.50   386.0 2024-12-19 15:57:00   \n",
       "9695763  12/19/2024  15:58:00  5941.50  5941.00   484.0 2024-12-19 15:58:00   \n",
       "9695764  12/19/2024  15:59:00  5940.75  5941.00  6462.0 2024-12-19 15:59:00   \n",
       "9695765  12/19/2024  16:00:00  5941.00  5941.25  8864.0 2024-12-19 16:00:00   \n",
       "\n",
       "        Date_surprise Period Event Ticker  ...  Day Surv(M) # Ests. Std Dev  \\\n",
       "2                 NaN    NaN   NaN    NaN  ...  NaN     NaN     NaN     NaN   \n",
       "3                 NaN    NaN   NaN    NaN  ...  NaN     NaN     NaN     NaN   \n",
       "4                 NaN    NaN   NaN    NaN  ...  NaN     NaN     NaN     NaN   \n",
       "5                 NaN    NaN   NaN    NaN  ...  NaN     NaN     NaN     NaN   \n",
       "6                 NaN    NaN   NaN    NaN  ...  NaN     NaN     NaN     NaN   \n",
       "...               ...    ...   ...    ...  ...  ...     ...     ...     ...   \n",
       "9695761           NaN    NaN   NaN    NaN  ...  NaN     NaN     NaN     NaN   \n",
       "9695762           NaN    NaN   NaN    NaN  ...  NaN     NaN     NaN     NaN   \n",
       "9695763           NaN    NaN   NaN    NaN  ...  NaN     NaN     NaN     NaN   \n",
       "9695764           NaN    NaN   NaN    NaN  ...  NaN     NaN     NaN     NaN   \n",
       "9695765           NaN    NaN   NaN    NaN  ...  NaN     NaN     NaN     NaN   \n",
       "\n",
       "        Surprise Country/Region Flag     _merge Surprise Occurred  \\\n",
       "2            NaN            NaN  NaN  left_only             False   \n",
       "3            NaN            NaN  NaN  left_only             False   \n",
       "4            NaN            NaN  NaN  left_only             False   \n",
       "5            NaN            NaN  NaN  left_only             False   \n",
       "6            NaN            NaN  NaN  left_only             False   \n",
       "...          ...            ...  ...        ...               ...   \n",
       "9695761      NaN            NaN  NaN  left_only             False   \n",
       "9695762      NaN            NaN  NaN  left_only             False   \n",
       "9695763      NaN            NaN  NaN  left_only             False   \n",
       "9695764      NaN            NaN  NaN  left_only             False   \n",
       "9695765      NaN            NaN  NaN  left_only             False   \n",
       "\n",
       "         First Post Surprise  \n",
       "2                      False  \n",
       "3                      False  \n",
       "4                      False  \n",
       "5                      False  \n",
       "6                      False  \n",
       "...                      ...  \n",
       "9695761                False  \n",
       "9695762                False  \n",
       "9695763                False  \n",
       "9695764                False  \n",
       "9695765                False  \n",
       "\n",
       "[9695665 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_es = pd.DataFrame()\n",
    "for i in range(1, 12):\n",
    "    df_temp = pd.read_csv(f'ES_part_{i}.csv')\n",
    "    df_es = pd.concat([df_es, df_temp], ignore_index=True)\n",
    "\n",
    "df_surprise = pd.read_csv('US_economic_releases_events.csv')\n",
    "\n",
    "df_surprise.drop(columns=['S', 'Month', 'Surv(A)', 'Surv(H)', 'Surv(L)',], inplace=True)\n",
    "\n",
    "df_surprise.drop(columns=['Flag', 'Country/Region', 'Day', 'C', 'Category','Subcategory', 'Std Dev', 'Period', 'Actual'])\n",
    "\n",
    "# Dropping all rows for which surprise column has NaN or 0 value\n",
    "df_surprise.dropna(subset=['Surprise'], inplace=True)\n",
    "df_surprise = df_surprise[df_surprise['Surprise'] != 0]\n",
    "\n",
    "df_surprise.replace(\"--\", pd.NA, inplace=True)\n",
    "\n",
    "# Redoing dropping all rows for which surprise column has NaN or 0 value\n",
    "df_surprise.dropna(subset=['Surprise'], inplace=True)\n",
    "df_surprise = df_surprise[df_surprise['Surprise'] != 0]\n",
    "\n",
    "# Convert 'Surprise' column to float\n",
    "df_surprise['Surprise'] = pd.to_numeric(df_surprise['Surprise'], errors='coerce')\n",
    "\n",
    "# Again filtering out rows where 'Surprise' is 0 or NaN\n",
    "df_surprise = df_surprise[df_surprise['Surprise'] != 0].dropna(subset=['Surprise'])\n",
    "\n",
    "df_surprise.dropna(subset=['Time'], inplace=True)\n",
    "\n",
    "# Wincorsizing to get results between 0.5% and 99.5% percentile for Surprise values\n",
    "lower_bound = df_surprise['Surprise'].quantile(0.005)\n",
    "upper_bound = df_surprise['Surprise'].quantile(0.995)\n",
    "\n",
    "df_surprise = df_surprise[(df_surprise['Surprise'] >= lower_bound) & (df_surprise['Surprise'] <= upper_bound)]\n",
    "\n",
    "# Step 1: Ensure columns are strings\n",
    "df_surprise['Date'] = df_surprise['Date'].astype(str)\n",
    "df_surprise['Time'] = df_surprise['Time'].astype(str)\n",
    "\n",
    "# Step 2: Handle missing times (if any)\n",
    "df_surprise['Time'] = df_surprise['Time'].fillna('00:00:00')\n",
    "\n",
    "# Step 3: Combine Date and Time into DateTime\n",
    "df_surprise['DateTime'] = pd.to_datetime(\n",
    "    df_surprise['Date'].str[:10] + ' ' + df_surprise['Time'],\n",
    "    format='%Y-%m-%d %H:%M:%S',\n",
    "    errors='coerce'  # Converts invalid parsing to NaT instead of raising error\n",
    ")\n",
    "\n",
    "# First we drop Date and DateTime and change the column name for Unnamed: 0 to Date\n",
    "\n",
    "df_surprise.drop(columns=['Date', 'DateTime'], inplace=True)\n",
    "df_surprise.rename(columns={'Unnamed: 0': 'Date'}, inplace=True)\n",
    "\n",
    "# Now let's again create a DateTime column with the new Date column and check for number of NaN values\n",
    "# Step 1: Ensure columns are strings\n",
    "df_surprise['Date'] = df_surprise['Date'].astype(str)\n",
    "df_surprise['Time'] = df_surprise['Time'].astype(str)\n",
    "\n",
    "# Step 2: Handle missing times (if any)\n",
    "df_surprise['Time'] = df_surprise['Time'].fillna('00:00:00')\n",
    "\n",
    "# Step 3: Combine Date and Time into DateTime\n",
    "df_surprise['DateTime'] = pd.to_datetime(\n",
    "    df_surprise['Date'].str[:10] + ' ' + df_surprise['Time'],\n",
    "    format='%Y-%m-%d %H:%M:%S',\n",
    "    errors='coerce'  # Converts invalid parsing to NaT instead of raising error\n",
    ")\n",
    "\n",
    "# Step 1: Ensure columns are strings\n",
    "df_es['Date'] = df_es['Date'].astype(str)\n",
    "df_es['Time'] = df_es['Time'].astype(str)\n",
    "\n",
    "# Step 2: Handle missing times (if any) and pad with seconds\n",
    "df_es['Time'] = df_es['Time'].fillna('00:00')  # Fill missing times\n",
    "df_es['Time'] = df_es['Time'] + ':00'  # Add seconds to make HH:MM:SS format\n",
    "\n",
    "# Step 3: Combine Date and Time into DateTime with correct format\n",
    "df_es['DateTime'] = pd.to_datetime(\n",
    "    df_es['Date'] + ' ' + df_es['Time'],\n",
    "    format='%m/%d/%Y %H:%M:%S',  # Matches MM/DD/YYYY date and HH:MM:SS time\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# Now we create the merged dataframe for our analysis - to allow us to match times of surprise with the price of the future at the time\n",
    "df_combined = pd.merge(\n",
    "    df_es,\n",
    "    df_surprise,\n",
    "    on='DateTime',\n",
    "    how='outer',\n",
    "    suffixes=('_es', '_surprise'),\n",
    "    indicator=True  # this shows the source of each using suffix\n",
    ")\n",
    "\n",
    "# Some surprise announcements might have come before the starting point for the data on the futures, these would be meaningless for our analysis and should thus\n",
    "# be removed by removing all rows with NaN values for Open\n",
    "\n",
    "df_combined.dropna(subset=['Open'], inplace=True)\n",
    "\n",
    "df_combined['Surprise Occurred'] = df_combined['Surprise'].notna()\n",
    "\n",
    "\n",
    "df_combined['First Post Surprise'] = df_combined['Surprise Occurred'].shift(1)\n",
    "df_combined['First Post Surprise'].iloc[0] = False\n",
    "\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60ff7162",
   "metadata": {},
   "outputs": [],
   "source": [
    "holding_period = 20\n",
    "transaction_cost = 0.000025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f59a2a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_es</th>\n",
       "      <th>Time_es</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Date_surprise</th>\n",
       "      <th>Period</th>\n",
       "      <th>Event</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>...</th>\n",
       "      <th>Flag</th>\n",
       "      <th>_merge</th>\n",
       "      <th>Surprise Occurred</th>\n",
       "      <th>First Post Surprise</th>\n",
       "      <th>N_Return</th>\n",
       "      <th>N_Return_half</th>\n",
       "      <th>N_Return_double</th>\n",
       "      <th>Return</th>\n",
       "      <th>Return_half</th>\n",
       "      <th>Return_double</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>09/10/1997</td>\n",
       "      <td>00:01:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1997-09-10 00:01:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>09/10/1997</td>\n",
       "      <td>00:02:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1997-09-10 00:02:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>09/10/1997</td>\n",
       "      <td>00:03:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1997-09-10 00:03:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>09/10/1997</td>\n",
       "      <td>00:04:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1997-09-10 00:04:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>09/10/1997</td>\n",
       "      <td>00:05:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1997-09-10 00:05:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9695761</th>\n",
       "      <td>12/19/2024</td>\n",
       "      <td>15:56:00</td>\n",
       "      <td>5941.75</td>\n",
       "      <td>5941.75</td>\n",
       "      <td>318.0</td>\n",
       "      <td>2024-12-19 15:56:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9695762</th>\n",
       "      <td>12/19/2024</td>\n",
       "      <td>15:57:00</td>\n",
       "      <td>5941.75</td>\n",
       "      <td>5941.50</td>\n",
       "      <td>386.0</td>\n",
       "      <td>2024-12-19 15:57:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9695763</th>\n",
       "      <td>12/19/2024</td>\n",
       "      <td>15:58:00</td>\n",
       "      <td>5941.50</td>\n",
       "      <td>5941.00</td>\n",
       "      <td>484.0</td>\n",
       "      <td>2024-12-19 15:58:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9695764</th>\n",
       "      <td>12/19/2024</td>\n",
       "      <td>15:59:00</td>\n",
       "      <td>5940.75</td>\n",
       "      <td>5941.00</td>\n",
       "      <td>6462.0</td>\n",
       "      <td>2024-12-19 15:59:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9695765</th>\n",
       "      <td>12/19/2024</td>\n",
       "      <td>16:00:00</td>\n",
       "      <td>5941.00</td>\n",
       "      <td>5941.25</td>\n",
       "      <td>8864.0</td>\n",
       "      <td>2024-12-19 16:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9695665 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date_es   Time_es     Open    Close  Volume            DateTime  \\\n",
       "2        09/10/1997  00:01:00     0.00     0.00     0.0 1997-09-10 00:01:00   \n",
       "3        09/10/1997  00:02:00     0.00     0.00     0.0 1997-09-10 00:02:00   \n",
       "4        09/10/1997  00:03:00     0.00     0.00     0.0 1997-09-10 00:03:00   \n",
       "5        09/10/1997  00:04:00     0.00     0.00     0.0 1997-09-10 00:04:00   \n",
       "6        09/10/1997  00:05:00     0.00     0.00     0.0 1997-09-10 00:05:00   \n",
       "...             ...       ...      ...      ...     ...                 ...   \n",
       "9695761  12/19/2024  15:56:00  5941.75  5941.75   318.0 2024-12-19 15:56:00   \n",
       "9695762  12/19/2024  15:57:00  5941.75  5941.50   386.0 2024-12-19 15:57:00   \n",
       "9695763  12/19/2024  15:58:00  5941.50  5941.00   484.0 2024-12-19 15:58:00   \n",
       "9695764  12/19/2024  15:59:00  5940.75  5941.00  6462.0 2024-12-19 15:59:00   \n",
       "9695765  12/19/2024  16:00:00  5941.00  5941.25  8864.0 2024-12-19 16:00:00   \n",
       "\n",
       "        Date_surprise Period Event Ticker  ... Flag     _merge  \\\n",
       "2                 NaN    NaN   NaN    NaN  ...  NaN  left_only   \n",
       "3                 NaN    NaN   NaN    NaN  ...  NaN  left_only   \n",
       "4                 NaN    NaN   NaN    NaN  ...  NaN  left_only   \n",
       "5                 NaN    NaN   NaN    NaN  ...  NaN  left_only   \n",
       "6                 NaN    NaN   NaN    NaN  ...  NaN  left_only   \n",
       "...               ...    ...   ...    ...  ...  ...        ...   \n",
       "9695761           NaN    NaN   NaN    NaN  ...  NaN  left_only   \n",
       "9695762           NaN    NaN   NaN    NaN  ...  NaN  left_only   \n",
       "9695763           NaN    NaN   NaN    NaN  ...  NaN  left_only   \n",
       "9695764           NaN    NaN   NaN    NaN  ...  NaN  left_only   \n",
       "9695765           NaN    NaN   NaN    NaN  ...  NaN  left_only   \n",
       "\n",
       "        Surprise Occurred First Post Surprise N_Return N_Return_half  \\\n",
       "2                   False               False      NaN           NaN   \n",
       "3                   False               False      NaN           NaN   \n",
       "4                   False               False      NaN           NaN   \n",
       "5                   False               False      NaN           NaN   \n",
       "6                   False               False      NaN           NaN   \n",
       "...                   ...                 ...      ...           ...   \n",
       "9695761             False               False      NaN           NaN   \n",
       "9695762             False               False      NaN           NaN   \n",
       "9695763             False               False      NaN           NaN   \n",
       "9695764             False               False      NaN           NaN   \n",
       "9695765             False               False      NaN           NaN   \n",
       "\n",
       "        N_Return_double Return Return_half  Return_double  \n",
       "2                   NaN    NaN         NaN            NaN  \n",
       "3                   NaN    NaN         NaN            NaN  \n",
       "4                   NaN    NaN         NaN            NaN  \n",
       "5                   NaN    NaN         NaN            NaN  \n",
       "6                   NaN    NaN         NaN            NaN  \n",
       "...                 ...    ...         ...            ...  \n",
       "9695761             NaN    NaN         NaN            NaN  \n",
       "9695762             NaN    NaN         NaN            NaN  \n",
       "9695763             NaN    NaN         NaN            NaN  \n",
       "9695764             NaN    NaN         NaN            NaN  \n",
       "9695765             NaN    NaN         NaN            NaN  \n",
       "\n",
       "[9695665 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined['N_Return'] = np.nan\n",
    "df_combined['N_Return_half'] = np.nan\n",
    "df_combined['N_Return_double'] = np.nan\n",
    "df_combined['Return'] = np.nan\n",
    "df_combined['Return_half'] = np.nan\n",
    "df_combined['Return_double'] = np.nan\n",
    "\n",
    "surprise_rows = df_combined[df_combined['First Post Surprise'] == True]\n",
    "\n",
    "for index, row in surprise_rows.iterrows():\n",
    "    row_position = df_combined.index.get_loc(index)\n",
    "\n",
    "    # Get the Surprise value from the previous row\n",
    "    if index > df_combined.index[0]:\n",
    "        prev_surprise = df_combined.at[df_combined.index[df_combined.index.get_loc(index)-1], 'Surprise']\n",
    "    else:\n",
    "        prev_surprise = 0  # Default if no previous row exists\n",
    "\n",
    "    if row_position + holding_period < len(df_combined):\n",
    "        current_price = row['Open']\n",
    "        future_row = df_combined.iloc[row_position + holding_period]\n",
    "        future_price = future_row['Open']\n",
    "\n",
    "        if current_price != 0:\n",
    "            percentage_increase = (future_price - current_price) / current_price\n",
    "        else:\n",
    "            percentage_increase = np.nan\n",
    "\n",
    "        df_combined.at[index, 'Return'] = percentage_increase\n",
    "        if prev_surprise < 0:\n",
    "            df_combined.at[index, 'N_Return'] = -percentage_increase\n",
    "        else:\n",
    "            df_combined.at[index, 'N_Return'] = percentage_increase\n",
    "\n",
    "    if row_position + int(holding_period*0.5) < len(df_combined): # int converts to whole number to avoid that potential error\n",
    "        current_price = row['Open']\n",
    "        future_row = df_combined.iloc[row_position + int(holding_period*0.5)]\n",
    "        future_price = future_row['Open']\n",
    "\n",
    "        if current_price != 0:\n",
    "            percentage_increase = (future_price - current_price) / current_price\n",
    "        else:\n",
    "            percentage_increase = np.nan\n",
    "\n",
    "        df_combined.at[index, 'Return_half'] = percentage_increase\n",
    "        if prev_surprise < 0:\n",
    "            df_combined.at[index, 'N_Return_half'] = -percentage_increase\n",
    "        else:\n",
    "            df_combined.at[index, 'N_Return_half'] = percentage_increase\n",
    "\n",
    "    if row_position + int(holding_period*2) < len(df_combined):\n",
    "        current_price = row['Open']\n",
    "        future_row = df_combined.iloc[row_position + int(holding_period*2)]\n",
    "        future_price = future_row['Open']\n",
    "\n",
    "        if current_price != 0:\n",
    "            percentage_increase = (future_price - current_price) / current_price\n",
    "        else:\n",
    "            percentage_increase = np.nan\n",
    "\n",
    "        df_combined.at[index, 'Return_double'] = percentage_increase\n",
    "        if prev_surprise < 0:\n",
    "            df_combined.at[index, 'N_Return_double'] = -percentage_increase\n",
    "        else:\n",
    "            df_combined.at[index, 'N_Return_double'] = percentage_increase\n",
    "\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb49e6c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myfinance\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myf\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msm\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# --- 1. Get GDP Release Dates (Quarterly) ---\u001b[39;00m\n\u001b[32m      9\u001b[39m start = datetime.datetime(\u001b[32m1997\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jop Brouwer\\Documents\\GitHub\\Data-Science-Seminar-Group-Project-5\\.venv\\Lib\\site-packages\\statsmodels\\api.py:76\u001b[39m\n\u001b[32m      1\u001b[39m __all__ = [\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mBayesGaussMI\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mBinomialBayesMixedGLM\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     72\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__version_info__\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datasets, distributions, iolib, regression, robust, tools\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__init__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m test\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_version\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     79\u001b[39m     version \u001b[38;5;28;01mas\u001b[39;00m __version__, version_tuple \u001b[38;5;28;01mas\u001b[39;00m __version_info__\n\u001b[32m     80\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jop Brouwer\\Documents\\GitHub\\Data-Science-Seminar-Group-Project-5\\.venv\\Lib\\site-packages\\statsmodels\\distributions\\__init__.py:5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_test_runner\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PytestTester\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mempirical_distribution\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      3\u001b[39m     ECDF, ECDFDiscrete, monotone_fn_inverter, StepFunction\n\u001b[32m      4\u001b[39m     )\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01medgeworth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExpandedNormal\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdiscrete\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      8\u001b[39m     genpoisson_p, zipoisson, zigenpoisson, zinegbin,\n\u001b[32m      9\u001b[39m     )\n\u001b[32m     11\u001b[39m __all__ = [\n\u001b[32m     12\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mECDF\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     13\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mECDFDiscrete\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mzipoisson\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     22\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jop Brouwer\\Documents\\GitHub\\Data-Science-Seminar-Group-Project-5\\.venv\\Lib\\site-packages\\statsmodels\\distributions\\edgeworth.py:6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpolynomial\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhermite_e\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HermiteE\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspecial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m factorial\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rv_continuous\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspecial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspecial\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# TODO:\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# * actually solve (31) of Blinnikov & Moessner\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# * numerical stability: multiply factorials in logspace?\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# * ppf & friends: Cornish & Fisher series, or tabulate/solve\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jop Brouwer\\Documents\\GitHub\\Data-Science-Seminar-Group-Project-5\\.venv\\Lib\\site-packages\\scipy\\stats\\__init__.py:618\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mstats\n\u001b[32m    617\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m qmc\n\u001b[32m--> \u001b[39m\u001b[32m618\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_multivariate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    619\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m contingency\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontingency\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m chi2_contingency\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jop Brouwer\\Documents\\GitHub\\Data-Science-Seminar-Group-Project-5\\.venv\\Lib\\site-packages\\scipy\\stats\\_multivariate.py:18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_discrete_distns\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m binom\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _mvn, _covariance, _rcont\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_qmvnt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _qmvt\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_morestats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m directional_stats\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptimize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m root_scalar\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1178\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1149\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:690\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:936\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1026\u001b[39m, in \u001b[36mget_code\u001b[39m\u001b[34m(self, fullname)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1148\u001b[39m, in \u001b[36mpath_stats\u001b[39m\u001b[34m(self, path)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:147\u001b[39m, in \u001b[36m_path_stat\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas_datareader import data as pdr\n",
    "import datetime\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# --- 1. Get GDP Release Dates (Quarterly) ---\n",
    "start = datetime.datetime(1997, 1, 1)\n",
    "end = datetime.datetime.today()\n",
    "gdp_releases = pdr.DataReader('A191RL1Q225SBEA', 'fred', start, end).index\n",
    "\n",
    "# --- 2. Download All Economic Data ---\n",
    "raw_data = {\n",
    "    # Quarterly\n",
    "    'gdp_gr': pdr.DataReader('A191RL1Q225SBEA', 'fred', start, end),\n",
    "\n",
    "    # Monthly\n",
    "    'NFP': pdr.DataReader('PAYEMS', 'fred', start, end),\n",
    "    'un_rate': pdr.DataReader('UNRATE', 'fred', start, end),\n",
    "    'cci': pdr.DataReader('UMCSENT', 'fred', start, end),\n",
    "    'ret_sales': pdr.DataReader('RSAFS', 'fred', start, end),\n",
    "    'IPTI': pdr.DataReader('INDPRO', 'fred', start, end),\n",
    "    'cpi': pdr.DataReader('CPIAUCSL', 'fred', start, end),\n",
    "    'TXBPPRIVSA': pdr.DataReader('TXBPPRIVSA', 'fred', start, end),  # Added this line\n",
    "\n",
    "    # Daily\n",
    "    'ffr': pdr.DataReader('DFF', 'fred', start, end),\n",
    "\n",
    "    # S&P 500 (via yfinance)\n",
    "    'sp500': yf.download('^GSPC', start=start, end=end)['Close']\n",
    "}\n",
    "\n",
    "# --- 3. Align ALL Variables to GDP Release Dates ---\n",
    "economic_data = pd.DataFrame(index=gdp_releases)\n",
    "\n",
    "for name, series in raw_data.items():\n",
    "    if name == 'gdp_gr':\n",
    "        economic_data[name] = series  # Already aligned\n",
    "    elif name == 'sp500':\n",
    "        # Calculate quarterly returns between GDP releases\n",
    "        prices_on_releases = series.reindex(gdp_releases, method='bfill')\n",
    "        economic_data['SP500'] = prices_on_releases.pct_change()\n",
    "    elif name == 'ffr':\n",
    "        # Take FFR value ON the GDP release day\n",
    "        economic_data[name] = series.reindex(gdp_releases, method='bfill')\n",
    "    else:\n",
    "        # Monthly series: last release BEFORE GDP date\n",
    "        economic_data[name] = series.reindex(gdp_releases, method='ffill')\n",
    "\n",
    "# --- 4. Clean and Verify ---\n",
    "economic_data = economic_data.dropna(subset=['gdp_gr'])  # Remove quarters without GDP\n",
    "\n",
    "# remove row where year is 2025\n",
    "economic_data = economic_data[economic_data.index.year != 2025]\n",
    "\n",
    "economic_data = economic_data.dropna()\n",
    "\n",
    "economic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bac350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gdp_gr</th>\n",
       "      <th>NFP</th>\n",
       "      <th>un_rate</th>\n",
       "      <th>cci</th>\n",
       "      <th>ret_sales</th>\n",
       "      <th>IPTI</th>\n",
       "      <th>cpi</th>\n",
       "      <th>TXBPPRIVSA</th>\n",
       "      <th>ffr</th>\n",
       "      <th>SP500</th>\n",
       "      <th>gdp_gr_L1</th>\n",
       "      <th>ffr_L1</th>\n",
       "      <th>NFP_L1</th>\n",
       "      <th>un_rate_L1</th>\n",
       "      <th>cci_L1</th>\n",
       "      <th>ret_sales_L1</th>\n",
       "      <th>IPTI_L1</th>\n",
       "      <th>cpi_L1</th>\n",
       "      <th>TXBPPRIVSA_L1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1997-07-01</th>\n",
       "      <td>5.1</td>\n",
       "      <td>123114</td>\n",
       "      <td>4.9</td>\n",
       "      <td>107.1</td>\n",
       "      <td>222985</td>\n",
       "      <td>80.4666</td>\n",
       "      <td>160.400</td>\n",
       "      <td>10146.266776</td>\n",
       "      <td>6.24</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>6.8</td>\n",
       "      <td>6.18</td>\n",
       "      <td>122288.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>101.4</td>\n",
       "      <td>217925.0</td>\n",
       "      <td>78.9848</td>\n",
       "      <td>159.900</td>\n",
       "      <td>10529.485939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-10-01</th>\n",
       "      <td>3.5</td>\n",
       "      <td>123924</td>\n",
       "      <td>4.7</td>\n",
       "      <td>105.6</td>\n",
       "      <td>224300</td>\n",
       "      <td>82.7656</td>\n",
       "      <td>161.500</td>\n",
       "      <td>10852.416694</td>\n",
       "      <td>5.65</td>\n",
       "      <td>0.072253</td>\n",
       "      <td>5.1</td>\n",
       "      <td>6.24</td>\n",
       "      <td>123114.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>107.1</td>\n",
       "      <td>222985.0</td>\n",
       "      <td>80.4666</td>\n",
       "      <td>160.400</td>\n",
       "      <td>10146.266776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-01-01</th>\n",
       "      <td>4.1</td>\n",
       "      <td>124806</td>\n",
       "      <td>4.6</td>\n",
       "      <td>106.6</td>\n",
       "      <td>225954</td>\n",
       "      <td>84.1564</td>\n",
       "      <td>162.000</td>\n",
       "      <td>9455.132516</td>\n",
       "      <td>5.84</td>\n",
       "      <td>0.020546</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.65</td>\n",
       "      <td>123924.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>105.6</td>\n",
       "      <td>224300.0</td>\n",
       "      <td>82.7656</td>\n",
       "      <td>161.500</td>\n",
       "      <td>10852.416694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-04-01</th>\n",
       "      <td>3.8</td>\n",
       "      <td>125446</td>\n",
       "      <td>4.3</td>\n",
       "      <td>108.7</td>\n",
       "      <td>230409</td>\n",
       "      <td>84.6339</td>\n",
       "      <td>162.200</td>\n",
       "      <td>12178.889660</td>\n",
       "      <td>5.72</td>\n",
       "      <td>0.136518</td>\n",
       "      <td>4.1</td>\n",
       "      <td>5.84</td>\n",
       "      <td>124806.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>106.6</td>\n",
       "      <td>225954.0</td>\n",
       "      <td>84.1564</td>\n",
       "      <td>162.000</td>\n",
       "      <td>9455.132516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-07-01</th>\n",
       "      <td>5.1</td>\n",
       "      <td>126211</td>\n",
       "      <td>4.5</td>\n",
       "      <td>105.2</td>\n",
       "      <td>231551</td>\n",
       "      <td>84.3445</td>\n",
       "      <td>163.200</td>\n",
       "      <td>13256.650856</td>\n",
       "      <td>6.35</td>\n",
       "      <td>0.036466</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.72</td>\n",
       "      <td>125446.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>108.7</td>\n",
       "      <td>230409.0</td>\n",
       "      <td>84.6339</td>\n",
       "      <td>162.200</td>\n",
       "      <td>12178.889660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-01</th>\n",
       "      <td>3.2</td>\n",
       "      <td>156520</td>\n",
       "      <td>3.9</td>\n",
       "      <td>63.8</td>\n",
       "      <td>686148</td>\n",
       "      <td>102.5781</td>\n",
       "      <td>307.653</td>\n",
       "      <td>19500.544159</td>\n",
       "      <td>5.33</td>\n",
       "      <td>-0.037526</td>\n",
       "      <td>4.4</td>\n",
       "      <td>5.08</td>\n",
       "      <td>156019.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>71.5</td>\n",
       "      <td>678424.0</td>\n",
       "      <td>103.0722</td>\n",
       "      <td>304.615</td>\n",
       "      <td>17947.555738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-01</th>\n",
       "      <td>1.6</td>\n",
       "      <td>157049</td>\n",
       "      <td>3.7</td>\n",
       "      <td>79.0</td>\n",
       "      <td>680456</td>\n",
       "      <td>101.4830</td>\n",
       "      <td>309.794</td>\n",
       "      <td>19795.233356</td>\n",
       "      <td>5.33</td>\n",
       "      <td>0.105970</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.33</td>\n",
       "      <td>156520.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>63.8</td>\n",
       "      <td>686148.0</td>\n",
       "      <td>102.5781</td>\n",
       "      <td>307.653</td>\n",
       "      <td>19500.544159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-01</th>\n",
       "      <td>3.0</td>\n",
       "      <td>157635</td>\n",
       "      <td>3.9</td>\n",
       "      <td>77.2</td>\n",
       "      <td>688913</td>\n",
       "      <td>102.3568</td>\n",
       "      <td>313.016</td>\n",
       "      <td>19904.874909</td>\n",
       "      <td>5.33</td>\n",
       "      <td>0.105620</td>\n",
       "      <td>1.6</td>\n",
       "      <td>5.33</td>\n",
       "      <td>157049.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>79.0</td>\n",
       "      <td>680456.0</td>\n",
       "      <td>101.4830</td>\n",
       "      <td>309.794</td>\n",
       "      <td>19795.233356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-01</th>\n",
       "      <td>3.1</td>\n",
       "      <td>158003</td>\n",
       "      <td>4.2</td>\n",
       "      <td>66.4</td>\n",
       "      <td>699098</td>\n",
       "      <td>102.5192</td>\n",
       "      <td>313.566</td>\n",
       "      <td>18899.758428</td>\n",
       "      <td>5.33</td>\n",
       "      <td>0.044113</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.33</td>\n",
       "      <td>157635.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>77.2</td>\n",
       "      <td>688913.0</td>\n",
       "      <td>102.3568</td>\n",
       "      <td>313.016</td>\n",
       "      <td>19904.874909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-01</th>\n",
       "      <td>2.4</td>\n",
       "      <td>158358</td>\n",
       "      <td>4.1</td>\n",
       "      <td>70.5</td>\n",
       "      <td>707613</td>\n",
       "      <td>102.2138</td>\n",
       "      <td>315.564</td>\n",
       "      <td>19370.476111</td>\n",
       "      <td>4.83</td>\n",
       "      <td>0.042677</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.33</td>\n",
       "      <td>158003.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>66.4</td>\n",
       "      <td>699098.0</td>\n",
       "      <td>102.5192</td>\n",
       "      <td>313.566</td>\n",
       "      <td>18899.758428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            gdp_gr     NFP  un_rate    cci  ret_sales      IPTI      cpi  \\\n",
       "DATE                                                                       \n",
       "1997-07-01     5.1  123114      4.9  107.1     222985   80.4666  160.400   \n",
       "1997-10-01     3.5  123924      4.7  105.6     224300   82.7656  161.500   \n",
       "1998-01-01     4.1  124806      4.6  106.6     225954   84.1564  162.000   \n",
       "1998-04-01     3.8  125446      4.3  108.7     230409   84.6339  162.200   \n",
       "1998-07-01     5.1  126211      4.5  105.2     231551   84.3445  163.200   \n",
       "...            ...     ...      ...    ...        ...       ...      ...   \n",
       "2023-10-01     3.2  156520      3.9   63.8     686148  102.5781  307.653   \n",
       "2024-01-01     1.6  157049      3.7   79.0     680456  101.4830  309.794   \n",
       "2024-04-01     3.0  157635      3.9   77.2     688913  102.3568  313.016   \n",
       "2024-07-01     3.1  158003      4.2   66.4     699098  102.5192  313.566   \n",
       "2024-10-01     2.4  158358      4.1   70.5     707613  102.2138  315.564   \n",
       "\n",
       "              TXBPPRIVSA   ffr     SP500  gdp_gr_L1  ffr_L1    NFP_L1  \\\n",
       "DATE                                                                    \n",
       "1997-07-01  10146.266776  6.24  0.172964        6.8    6.18  122288.0   \n",
       "1997-10-01  10852.416694  5.65  0.072253        5.1    6.24  123114.0   \n",
       "1998-01-01   9455.132516  5.84  0.020546        3.5    5.65  123924.0   \n",
       "1998-04-01  12178.889660  5.72  0.136518        4.1    5.84  124806.0   \n",
       "1998-07-01  13256.650856  6.35  0.036466        3.8    5.72  125446.0   \n",
       "...                  ...   ...       ...        ...     ...       ...   \n",
       "2023-10-01  19500.544159  5.33 -0.037526        4.4    5.08  156019.0   \n",
       "2024-01-01  19795.233356  5.33  0.105970        3.2    5.33  156520.0   \n",
       "2024-04-01  19904.874909  5.33  0.105620        1.6    5.33  157049.0   \n",
       "2024-07-01  18899.758428  5.33  0.044113        3.0    5.33  157635.0   \n",
       "2024-10-01  19370.476111  4.83  0.042677        3.1    5.33  158003.0   \n",
       "\n",
       "            un_rate_L1  cci_L1  ret_sales_L1   IPTI_L1   cpi_L1  TXBPPRIVSA_L1  \n",
       "DATE                                                                            \n",
       "1997-07-01         5.1   101.4      217925.0   78.9848  159.900   10529.485939  \n",
       "1997-10-01         4.9   107.1      222985.0   80.4666  160.400   10146.266776  \n",
       "1998-01-01         4.7   105.6      224300.0   82.7656  161.500   10852.416694  \n",
       "1998-04-01         4.6   106.6      225954.0   84.1564  162.000    9455.132516  \n",
       "1998-07-01         4.3   108.7      230409.0   84.6339  162.200   12178.889660  \n",
       "...                ...     ...           ...       ...      ...            ...  \n",
       "2023-10-01         3.5    71.5      678424.0  103.0722  304.615   17947.555738  \n",
       "2024-01-01         3.9    63.8      686148.0  102.5781  307.653   19500.544159  \n",
       "2024-04-01         3.7    79.0      680456.0  101.4830  309.794   19795.233356  \n",
       "2024-07-01         3.9    77.2      688913.0  102.3568  313.016   19904.874909  \n",
       "2024-10-01         4.2    66.4      699098.0  102.5192  313.566   18899.758428  \n",
       "\n",
       "[110 rows x 19 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "economic_data['gdp_gr_L1'] = economic_data['gdp_gr'].shift(1)\n",
    "economic_data['ffr_L1'] = economic_data['ffr'].shift(1)\n",
    "economic_data['NFP_L1'] = economic_data['NFP'].shift(1)\n",
    "economic_data['un_rate_L1'] = economic_data['un_rate'].shift(1)\n",
    "economic_data['cci_L1'] = economic_data['cci'].shift(1)\n",
    "economic_data['ret_sales_L1'] = economic_data['ret_sales'].shift(1)\n",
    "economic_data['IPTI_L1'] = economic_data['IPTI'].shift(1)\n",
    "economic_data['cpi_L1'] = economic_data['cpi'].shift(1)\n",
    "economic_data['TXBPPRIVSA_L1'] = economic_data['TXBPPRIVSA'].shift(1)\n",
    "\n",
    "economic_data = economic_data.dropna()\n",
    "\n",
    "economic_data.index = pd.to_datetime(economic_data.index)\n",
    "\n",
    "economic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a912abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_20832\\2640991562.py:1: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_surprise = pd.read_csv('US_economic_releases_events (3).csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>All Blomberg indicators</th>\n",
       "      <th>Description</th>\n",
       "      <th>Broad indicator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VNCCTOT Index</td>\n",
       "      <td>Vehicle Sales Total</td>\n",
       "      <td>Retail sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PHUCCHNG Index</td>\n",
       "      <td>Pending Home Sales Change</td>\n",
       "      <td>Building permits - new private housing units a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CICRTOT Index</td>\n",
       "      <td>Construction Spending Total</td>\n",
       "      <td>GDP growth rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MWINCHNG Index</td>\n",
       "      <td>MBA Mortgage Applications Change</td>\n",
       "      <td>Building permits - new private housing units a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NFP TCH Index</td>\n",
       "      <td>Nonfarm Payrolls Total Change</td>\n",
       "      <td>Non-farm payrolls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>NYCNM1IR Index</td>\n",
       "      <td>NY Fed: Empire State Manufacturing Index</td>\n",
       "      <td>Industrial production</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>DOTDY0MD Index</td>\n",
       "      <td>Department of Transportation: Domestic Air Rev...</td>\n",
       "      <td>GDP growth rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>DOTDY1MD Index</td>\n",
       "      <td>Department of Transportation: International Ai...</td>\n",
       "      <td>GDP growth rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>DOTDY2MD Index</td>\n",
       "      <td>Department of Transportation: Systemwide Air R...</td>\n",
       "      <td>GDP growth rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>DOTDLTMD Index</td>\n",
       "      <td>Department of Transportation: Load Factor</td>\n",
       "      <td>GDP growth rate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>161 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    All Blomberg indicators  \\\n",
       "0             VNCCTOT Index   \n",
       "1            PHUCCHNG Index   \n",
       "2             CICRTOT Index   \n",
       "3            MWINCHNG Index   \n",
       "4             NFP TCH Index   \n",
       "..                      ...   \n",
       "156          NYCNM1IR Index   \n",
       "157          DOTDY0MD Index   \n",
       "158          DOTDY1MD Index   \n",
       "159          DOTDY2MD Index   \n",
       "160          DOTDLTMD Index   \n",
       "\n",
       "                                           Description  \\\n",
       "0                                  Vehicle Sales Total   \n",
       "1                            Pending Home Sales Change   \n",
       "2                          Construction Spending Total   \n",
       "3                     MBA Mortgage Applications Change   \n",
       "4                        Nonfarm Payrolls Total Change   \n",
       "..                                                 ...   \n",
       "156           NY Fed: Empire State Manufacturing Index   \n",
       "157  Department of Transportation: Domestic Air Rev...   \n",
       "158  Department of Transportation: International Ai...   \n",
       "159  Department of Transportation: Systemwide Air R...   \n",
       "160          Department of Transportation: Load Factor   \n",
       "\n",
       "                                       Broad indicator  \n",
       "0                                         Retail sales  \n",
       "1    Building permits - new private housing units a...  \n",
       "2                                      GDP growth rate  \n",
       "3    Building permits - new private housing units a...  \n",
       "4                                    Non-farm payrolls  \n",
       "..                                                 ...  \n",
       "156                              Industrial production  \n",
       "157                                    GDP growth rate  \n",
       "158                                    GDP growth rate  \n",
       "159                                    GDP growth rate  \n",
       "160                                    GDP growth rate  \n",
       "\n",
       "[161 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_surprise = pd.read_csv('US_economic_releases_events (3).csv')\n",
    "\n",
    "df_mapping = pd.read_excel('US Variable mapping (1).xlsx')\n",
    "\n",
    "df_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e120e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Description</th>\n",
       "      <th>Broad indicator</th>\n",
       "      <th>Date_es</th>\n",
       "      <th>Time_es</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Date_surprise</th>\n",
       "      <th>...</th>\n",
       "      <th>Flag</th>\n",
       "      <th>_merge</th>\n",
       "      <th>Surprise Occurred</th>\n",
       "      <th>First Post Surprise</th>\n",
       "      <th>N_Return</th>\n",
       "      <th>N_Return_half</th>\n",
       "      <th>N_Return_double</th>\n",
       "      <th>Return</th>\n",
       "      <th>Return_half</th>\n",
       "      <th>Return_double</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23088</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09/10/1997</td>\n",
       "      <td>00:01:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1997-09-10 00:01:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23089</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09/10/1997</td>\n",
       "      <td>00:02:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1997-09-10 00:02:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23090</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09/10/1997</td>\n",
       "      <td>00:03:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1997-09-10 00:03:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23091</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09/10/1997</td>\n",
       "      <td>00:04:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1997-09-10 00:04:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23092</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09/10/1997</td>\n",
       "      <td>00:05:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1997-09-10 00:05:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17882</th>\n",
       "      <td>PHUCCHNG Index</td>\n",
       "      <td>Pending Home Sales Change</td>\n",
       "      <td>Building permits - new private housing units a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19078</th>\n",
       "      <td>RCSSCLBC Index</td>\n",
       "      <td>Retail Sales Control Group</td>\n",
       "      <td>Retail sales</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19096</th>\n",
       "      <td>REALYRAW Index</td>\n",
       "      <td>Real Personal Consumption Expenditures</td>\n",
       "      <td>GDP growth rate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19097</th>\n",
       "      <td>REDSMMOM Index</td>\n",
       "      <td>Redbook Same-Store Sales Monthly Change</td>\n",
       "      <td>Retail sales</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23087</th>\n",
       "      <td>VNCCTOT Index</td>\n",
       "      <td>Vehicle Sales Total</td>\n",
       "      <td>Retail sales</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9695685 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Ticker                              Description  \\\n",
       "23088             NaN                                      NaN   \n",
       "23089             NaN                                      NaN   \n",
       "23090             NaN                                      NaN   \n",
       "23091             NaN                                      NaN   \n",
       "23092             NaN                                      NaN   \n",
       "...               ...                                      ...   \n",
       "17882  PHUCCHNG Index                Pending Home Sales Change   \n",
       "19078  RCSSCLBC Index               Retail Sales Control Group   \n",
       "19096  REALYRAW Index   Real Personal Consumption Expenditures   \n",
       "19097  REDSMMOM Index  Redbook Same-Store Sales Monthly Change   \n",
       "23087   VNCCTOT Index                      Vehicle Sales Total   \n",
       "\n",
       "                                         Broad indicator     Date_es  \\\n",
       "23088                                                NaN  09/10/1997   \n",
       "23089                                                NaN  09/10/1997   \n",
       "23090                                                NaN  09/10/1997   \n",
       "23091                                                NaN  09/10/1997   \n",
       "23092                                                NaN  09/10/1997   \n",
       "...                                                  ...         ...   \n",
       "17882  Building permits - new private housing units a...         NaN   \n",
       "19078                                       Retail sales         NaN   \n",
       "19096                                    GDP growth rate         NaN   \n",
       "19097                                       Retail sales         NaN   \n",
       "23087                                       Retail sales         NaN   \n",
       "\n",
       "        Time_es  Open  Close  Volume            DateTime Date_surprise  ...  \\\n",
       "23088  00:01:00   0.0    0.0     0.0 1997-09-10 00:01:00           NaN  ...   \n",
       "23089  00:02:00   0.0    0.0     0.0 1997-09-10 00:02:00           NaN  ...   \n",
       "23090  00:03:00   0.0    0.0     0.0 1997-09-10 00:03:00           NaN  ...   \n",
       "23091  00:04:00   0.0    0.0     0.0 1997-09-10 00:04:00           NaN  ...   \n",
       "23092  00:05:00   0.0    0.0     0.0 1997-09-10 00:05:00           NaN  ...   \n",
       "...         ...   ...    ...     ...                 ...           ...  ...   \n",
       "17882       NaN   NaN    NaN     NaN                 NaT           NaN  ...   \n",
       "19078       NaN   NaN    NaN     NaN                 NaT           NaN  ...   \n",
       "19096       NaN   NaN    NaN     NaN                 NaT           NaN  ...   \n",
       "19097       NaN   NaN    NaN     NaN                 NaT           NaN  ...   \n",
       "23087       NaN   NaN    NaN     NaN                 NaT           NaN  ...   \n",
       "\n",
       "      Flag     _merge Surprise Occurred First Post Surprise N_Return  \\\n",
       "23088  NaN  left_only             False               False      NaN   \n",
       "23089  NaN  left_only             False               False      NaN   \n",
       "23090  NaN  left_only             False               False      NaN   \n",
       "23091  NaN  left_only             False               False      NaN   \n",
       "23092  NaN  left_only             False               False      NaN   \n",
       "...    ...        ...               ...                 ...      ...   \n",
       "17882  NaN        NaN               NaN                 NaN      NaN   \n",
       "19078  NaN        NaN               NaN                 NaN      NaN   \n",
       "19096  NaN        NaN               NaN                 NaN      NaN   \n",
       "19097  NaN        NaN               NaN                 NaN      NaN   \n",
       "23087  NaN        NaN               NaN                 NaN      NaN   \n",
       "\n",
       "      N_Return_half N_Return_double Return Return_half Return_double  \n",
       "23088           NaN             NaN    NaN         NaN           NaN  \n",
       "23089           NaN             NaN    NaN         NaN           NaN  \n",
       "23090           NaN             NaN    NaN         NaN           NaN  \n",
       "23091           NaN             NaN    NaN         NaN           NaN  \n",
       "23092           NaN             NaN    NaN         NaN           NaN  \n",
       "...             ...             ...    ...         ...           ...  \n",
       "17882           NaN             NaN    NaN         NaN           NaN  \n",
       "19078           NaN             NaN    NaN         NaN           NaN  \n",
       "19096           NaN             NaN    NaN         NaN           NaN  \n",
       "19097           NaN             NaN    NaN         NaN           NaN  \n",
       "23087           NaN             NaN    NaN         NaN           NaN  \n",
       "\n",
       "[9695685 rows x 39 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mapping = df_mapping.rename(columns={'All Blomberg indicators': 'Ticker'})\n",
    "\n",
    "df_combined = pd.merge(df_mapping, df_combined, on='Ticker', how='outer')\n",
    "\n",
    "df_combined = df_combined.sort_values(by='DateTime', ascending=True)\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1528f321",
   "metadata": {},
   "outputs": [],
   "source": [
    "economic_data = economic_data.copy()\n",
    "economic_data['DateTime'] = pd.to_datetime(economic_data.index).floor('D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f606d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Profit measure\n",
    "df_combined['Profit'] = np.where(\n",
    "    np.isnan(df_combined['Return']),\n",
    "    np.nan,  # If Return is NaN, set Profit to NaN\n",
    "    np.where(\n",
    "        df_combined['Return'] > 2 * transaction_cost,\n",
    "        1,  # Profitable long\n",
    "        np.where(\n",
    "            -df_combined['Return'] > 2 * transaction_cost,\n",
    "            2,  # Profitable short\n",
    "            0    # Neutral (within cost bounds)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# For Profit_half measure\n",
    "df_combined['Profit_half'] = np.where(\n",
    "    np.isnan(df_combined['Return_half']),\n",
    "    np.nan,\n",
    "    np.where(\n",
    "        df_combined['Return_half'] > 2 * transaction_cost,\n",
    "        1,\n",
    "        np.where(\n",
    "            -df_combined['Return_half'] > 2 * transaction_cost,\n",
    "            2,\n",
    "            0\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# For Profit_double measure\n",
    "df_combined['Profit_double'] = np.where(\n",
    "    np.isnan(df_combined['Return_double']),\n",
    "    np.nan,\n",
    "    np.where(\n",
    "        df_combined['Return_double'] > 2 * transaction_cost,\n",
    "        1,\n",
    "        np.where(\n",
    "            -df_combined['Return_double'] > 2 * transaction_cost,\n",
    "            2,\n",
    "            0\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23332417",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intm = df_combined[df_combined['Surprise Occurred'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce36d6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['Return'] = df_combined['Return'].shift(-1)\n",
    "df_combined['Return_half'] = df_combined['Return_half'].shift(-1)\n",
    "df_combined['Return_double'] = df_combined['Return_double'].shift(-1)\n",
    "\n",
    "df_combined['Profit'] = df_combined['Profit'].shift(-1)\n",
    "df_combined['Profit_half'] = df_combined['Profit_half'].shift(-1)\n",
    "df_combined['Profit_double'] = df_combined['Profit_double'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899e6bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_20832\\1017602337.py:25: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  df[f'UpDown{period}'] = np.sign(df['Open'].pct_change(period))\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_20832\\1017602337.py:25: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  df[f'UpDown{period}'] = np.sign(df['Open'].pct_change(period))\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_20832\\1017602337.py:25: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  df[f'UpDown{period}'] = np.sign(df['Open'].pct_change(period))\n"
     ]
    }
   ],
   "source": [
    "# Now we create the technical explanatory variables based on literature\n",
    "\n",
    "def create_technical_features(df):\n",
    "    \"\"\"\n",
    "    Creates all technical features for a dataframe containing price/volume data\n",
    "    and three return columns (Return, Return_half, Return_double)\n",
    "    \"\"\"\n",
    "    # 1. Original Features\n",
    "    df['Volume'] = df['Volume']\n",
    "    df['Price'] = df['Open']\n",
    "\n",
    "    # 2. Simple Moving Averages (now includes all required windows)\n",
    "    ma_windows = [5, 10, 15, 20, 50, 100, 200]  # Added missing windows for crossovers\n",
    "    for window in ma_windows:\n",
    "        df[f'SMA{window}'] = df['Open'].rolling(window).mean()\n",
    "\n",
    "    # 3. Moving Average Crossovers (now all SMAs exist)\n",
    "    for window in [5, 10, 15, 20, 50, 100, 200]:\n",
    "        # No more need for existence check since we created all SMAs\n",
    "        df[f'SMA{window}Cross'] = (df['Open'] > df[f'SMA{window}']).astype(int)\n",
    "\n",
    "    # 4. Consecutive Price Trends\n",
    "    trend_periods = [10, 15, 50]\n",
    "    for period in trend_periods:\n",
    "        df[f'UpDown{period}'] = np.sign(df['Open'].pct_change(period))\n",
    "\n",
    "    # Ensure we keep the original return columns\n",
    "    return_cols = ['Return', 'Return_half', 'Return_double']\n",
    "    for col in return_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col]  # Maintain existing returns\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# df_combined must contain: 'volume', 'close' columns plus the 3 return columns\n",
    "df_combined = create_technical_features(df_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43062b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Ticker', 'Description', 'Broad indicator', 'Date_es', 'Time_es',\n",
       "       'Open', 'Close', 'Volume', 'DateTime', 'Date_surprise', 'Period',\n",
       "       'Event', 'Actual', 'Prior', 'Revised', 'Freq.', 'First Rev.',\n",
       "       'Last Rev.', 'Time_surprise', 'C', 'Category', 'Subcategory', 'R',\n",
       "       'Day', 'Surv(M)', '# Ests.', 'Std Dev', 'Surprise', 'Country/Region',\n",
       "       'Flag', '_merge', 'Surprise Occurred', 'First Post Surprise',\n",
       "       'N_Return', 'N_Return_half', 'N_Return_double', 'Return', 'Return_half',\n",
       "       'Return_double', 'Profit', 'Profit_half', 'Profit_double', 'Price',\n",
       "       'SMA5', 'SMA10', 'SMA15', 'SMA20', 'SMA50', 'SMA100', 'SMA200',\n",
       "       'SMA5Cross', 'SMA10Cross', 'SMA15Cross', 'SMA20Cross', 'SMA50Cross',\n",
       "       'SMA100Cross', 'SMA200Cross', 'UpDown10', 'UpDown15', 'UpDown50',\n",
       "       'Volume_L1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined['Volume_L1'] = df_combined['Volume'].shift(1)\n",
    "\n",
    "df_combined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7a7d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the Ticker column\n",
    "ticker_dummies = pd.get_dummies(df_combined['Ticker'], prefix='Ticker')\n",
    "\n",
    "# Create new numerical ticker representation\n",
    "df_combined = pd.concat([df_combined, ticker_dummies], axis=1)\n",
    "\n",
    "# Create list of all ticker dummy columns\n",
    "ticker_cols = [col for col in df_combined.columns if col.startswith('Ticker_')]\n",
    "\n",
    "# Update feature columns to use dummy variables instead of original Ticker\n",
    "feature_cols = [\n",
    "    'Surprise', 'Volume_L1', 'R','SMA5', 'SMA10',\n",
    "    'SMA20', 'SMA200', 'SMA5Cross', 'SMA10Cross','SMA15Cross',\n",
    "    'SMA20Cross', 'SMA50Cross', 'SMA100Cross', 'SMA200Cross', 'UpDown10',\n",
    "    'UpDown15', 'UpDown50'\n",
    "] + ticker_cols  # Add the one-hot encoded ticker columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47554fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only rows where Profit label is available\n",
    "df_ml = df_combined[df_combined['Profit'].notna()].copy()\n",
    "df_ml = df_ml.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5019d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize columns\n",
    "df_combined['Total_Return'] = np.nan\n",
    "df_combined['Total_Return_half'] = np.nan\n",
    "df_combined['Total_Return_double'] = np.nan\n",
    "\n",
    "# Loop through all rows (not just surprise rows)\n",
    "for index, row in df_combined.iterrows():\n",
    "    row_position = df_combined.index.get_loc(index)\n",
    "\n",
    "    # Standard holding period\n",
    "    if row_position + holding_period < len(df_combined):\n",
    "        current_price = row['Open']\n",
    "        future_price = df_combined.iloc[row_position + holding_period]['Open']\n",
    "\n",
    "        if current_price != 0:\n",
    "            df_combined.at[index, 'Total_Return'] = (future_price - current_price) / current_price\n",
    "\n",
    "    # Half holding period\n",
    "    if row_position + int(holding_period * 0.5) < len(df_combined):\n",
    "        current_price = row['Open']\n",
    "        future_price = df_combined.iloc[row_position + int(holding_period * 0.5)]['Open']\n",
    "\n",
    "        if current_price != 0:\n",
    "            df_combined.at[index, 'Total_Return_half'] = (future_price - current_price) / current_price\n",
    "\n",
    "    # Double holding period\n",
    "    if row_position + int(holding_period * 2) < len(df_combined):\n",
    "        current_price = row['Open']\n",
    "        future_price = df_combined.iloc[row_position + int(holding_period * 2)]['Open']\n",
    "\n",
    "        if current_price != 0:\n",
    "            df_combined.at[index, 'Total_Return_double'] = (future_price - current_price) / current_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67440a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import pandas_datareader as pdr\n",
    "\n",
    "# --- 1. Set Start and End Dates ---\n",
    "start = datetime.datetime(1997, 1, 1)\n",
    "end = datetime.datetime.today()\n",
    "\n",
    "# --- 2. Get GDP Growth Data ---\n",
    "gdp_gr = pdr.DataReader('A191RL1Q225SBEA', 'fred', start, end)\n",
    "\n",
    "# Create gdp_gr_ml dataframe\n",
    "gdp_gr_ml = gdp_gr.reset_index()\n",
    "gdp_gr_ml.rename(columns={'A191RL1Q225SBEA': 'gdp_gr'}, inplace=True)\n",
    "gdp_gr_ml['DateTime'] = pd.to_datetime(gdp_gr_ml['DATE']) + pd.Timedelta(hours=23, minutes=59)\n",
    "gdp_gr_ml = gdp_gr_ml[['DateTime', 'gdp_gr']]\n",
    "\n",
    "# --- 3. Get VIX Data ---\n",
    "vix = pdr.DataReader('VIXCLS', 'fred', start, end)\n",
    "\n",
    "# Create vix_ml dataframe\n",
    "vix_ml = vix.reset_index()\n",
    "vix_ml.rename(columns={'VIXCLS': 'VIX'}, inplace=True)\n",
    "vix_ml['DateTime'] = pd.to_datetime(vix_ml['DATE']) + pd.Timedelta(hours=23, minutes=59)\n",
    "vix_ml = vix_ml[['DateTime', 'VIX']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d39835",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_gr_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677051af",
   "metadata": {},
   "outputs": [],
   "source": [
    "vix_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2c3012",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fe3179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First make sure all are sorted by DateTime (required for merge_asof)\n",
    "df_ml = df_ml.sort_values('DateTime')\n",
    "gdp_gr_ml = gdp_gr_ml.sort_values('DateTime')\n",
    "vix_ml = vix_ml.sort_values('DateTime')\n",
    "\n",
    "# Merge the last known GDP growth\n",
    "df_ml = pd.merge_asof(\n",
    "    df_ml,\n",
    "    gdp_gr_ml,\n",
    "    on='DateTime',\n",
    "    direction='backward'\n",
    ")\n",
    "\n",
    "# Merge the last known VIX\n",
    "df_ml = pd.merge_asof(\n",
    "    df_ml,\n",
    "    vix_ml,\n",
    "    on='DateTime',\n",
    "    direction='backward'\n",
    ")\n",
    "\n",
    "# Rename the merged columns to last_gdp_gr and last_vix\n",
    "df_ml = df_ml.rename(columns={\n",
    "    'gdp_gr': 'last_gdp_gr',\n",
    "    'VIX': 'last_vix'\n",
    "})\n",
    "\n",
    "df_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a511af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining new input features for ML models\n",
    "\n",
    "feature_cols = [\n",
    "    'Surprise', 'Volume_L1', 'R', 'SMA5', 'SMA10',\n",
    "    'SMA20', 'SMA200', 'SMA5Cross', 'SMA10Cross', 'SMA15Cross',\n",
    "    'SMA20Cross', 'SMA50Cross', 'SMA100Cross', 'SMA200Cross', 'UpDown10',\n",
    "    'UpDown15', 'UpDown50'\n",
    "] + ticker_cols + ['last_gdp_gr', 'last_vix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b5c6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af35ade",
   "metadata": {},
   "source": [
    "<h1>Final XGBoost</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fe8422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['VECLIB_MAXIMUM_THREADS'] = '1'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# --- Configuration ---\n",
    "total_cost = 2 * transaction_cost\n",
    "horizons = ['', '_half', '_double']\n",
    "\n",
    "# --- Data Prep ---\n",
    "df_ml = df_ml.dropna(subset=['Surprise']).sort_values('DateTime')\n",
    "min_date = df_ml['DateTime'].min()\n",
    "max_date = df_ml['DateTime'].max()\n",
    "\n",
    "# --- Calculate Initial 30% Training Period ---\n",
    "total_duration = max_date - min_date\n",
    "initial_train_duration = total_duration * 0.3\n",
    "train_end_date = min_date + initial_train_duration\n",
    "\n",
    "# --- Tracking containers ---\n",
    "results = []\n",
    "all_test_returns = []\n",
    "all_dates = []\n",
    "all_test_preds = []\n",
    "all_test_truths = []\n",
    "all_test_probas = []\n",
    "fold_mean_returns = []\n",
    "best_horizons_list = []\n",
    "\n",
    "def calculate_grouped_returns(df_group, horizon, stop_loss=None):\n",
    "    \"\"\"Calculate returns with position sizing and stop loss\"\"\"\n",
    "    active_trades = df_group[df_group['pred'] != 0]\n",
    "    if active_trades.empty:\n",
    "        return 0.0\n",
    "\n",
    "    longs = active_trades[active_trades['pred'] == 1]\n",
    "    shorts = active_trades[active_trades['pred'] == 2]\n",
    "\n",
    "    total_confidence = longs['proba'].sum() + shorts['proba'].sum()\n",
    "    if total_confidence == 0:\n",
    "        return 0.0\n",
    "\n",
    "    num_trades = len(longs) + len(shorts)\n",
    "    total_costs = total_cost * num_trades\n",
    "\n",
    "    long_returns = 0.0\n",
    "    short_returns = 0.0\n",
    "\n",
    "    # Process longs\n",
    "    if not longs.empty:\n",
    "        long_weights = longs['proba'] / total_confidence\n",
    "        for idx, row in longs.iterrows():\n",
    "            raw_return = row[f'Return{horizon}']\n",
    "            capped_raw = max(stop_loss, raw_return) if stop_loss is not None else raw_return\n",
    "            long_returns += capped_raw * long_weights.loc[idx]\n",
    "\n",
    "    # Process shorts\n",
    "    if not shorts.empty:\n",
    "        short_weights = shorts['proba'] / total_confidence\n",
    "        for idx, row in shorts.iterrows():\n",
    "            raw_return = -row[f'Return{horizon}']\n",
    "            capped_raw = max(stop_loss, raw_return) if stop_loss is not None else raw_return\n",
    "            short_returns += capped_raw * short_weights.loc[idx]\n",
    "\n",
    "    net_return = (long_returns + short_returns) - total_costs\n",
    "    return net_return\n",
    "\n",
    "# --- Main Loop ---\n",
    "while True:\n",
    "    val_start_date = train_end_date + pd.DateOffset(days=1)\n",
    "    val_end_date = val_start_date + pd.DateOffset(years=1)\n",
    "    test_start_date = val_end_date + pd.DateOffset(days=1)\n",
    "    test_end_date = test_start_date + pd.DateOffset(years=1)\n",
    "\n",
    "    if val_end_date > max_date or test_end_date > max_date:\n",
    "        break\n",
    "\n",
    "    train_mask = df_ml['DateTime'] <= train_end_date\n",
    "    val_mask = (df_ml['DateTime'] >= val_start_date) & (df_ml['DateTime'] <= val_end_date)\n",
    "    test_mask = (df_ml['DateTime'] >= test_start_date) & (df_ml['DateTime'] <= test_end_date)\n",
    "\n",
    "    train_data = df_ml[train_mask]\n",
    "    val_data = df_ml[val_mask]\n",
    "    test_data = df_ml[test_mask]\n",
    "\n",
    "    if val_data.empty or test_data.empty:\n",
    "        print(f\"Skipping period {val_start_date.date()} to {test_end_date.date()} (no data)\")\n",
    "        train_end_date += pd.DateOffset(years=1)\n",
    "        continue\n",
    "\n",
    "    # --- Horizon Optimization ---\n",
    "    best_horizon, best_model, best_stop_loss = None, None, None\n",
    "    best_val_return = -np.inf\n",
    "\n",
    "    for horizon in horizons:\n",
    "        # Feature scaling\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(train_data[feature_cols])\n",
    "        X_val = scaler.transform(val_data[feature_cols])\n",
    "        y_train = train_data[f'Profit{horizon}']\n",
    "        y_val = val_data[f'Profit{horizon}']\n",
    "\n",
    "        # Train model\n",
    "        model = xgb.XGBClassifier(\n",
    "            num_class=3,\n",
    "            n_estimators=60,\n",
    "            max_depth=7,\n",
    "            random_state=42,\n",
    "            seed=42\n",
    "        )\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "        # Calculate training stop loss\n",
    "        train_preds = model.predict(X_train)\n",
    "        train_trade_df = pd.DataFrame({\n",
    "            'DateTime': train_data['DateTime'],\n",
    "            'pred': train_preds,\n",
    "            'proba': [p[pred] for pred, p in zip(train_preds, model.predict_proba(X_train))],\n",
    "            f'Return{horizon}': train_data[f'Return{horizon}']\n",
    "        })\n",
    "\n",
    "        individual_train_returns = []\n",
    "        for _, row in train_trade_df.iterrows():\n",
    "            if row['pred'] == 1:\n",
    "                raw = row[f'Return{horizon}']\n",
    "            elif row['pred'] == 2:\n",
    "                raw = -row[f'Return{horizon}']\n",
    "            else:\n",
    "                continue\n",
    "            individual_train_returns.append(raw)\n",
    "\n",
    "        stop_loss_train = np.percentile(individual_train_returns, 5) if individual_train_returns else None\n",
    "\n",
    "        # Calculate validation stop loss (for test set)\n",
    "        val_preds = model.predict(X_val)\n",
    "        val_probas = model.predict_proba(X_val)\n",
    "        val_trade_df = pd.DataFrame({\n",
    "            'DateTime': val_data['DateTime'],\n",
    "            'pred': val_preds,\n",
    "            'proba': [p[pred] for pred, p in zip(val_preds, val_probas)],\n",
    "            f'Return{horizon}': val_data[f'Return{horizon}']\n",
    "        })\n",
    "\n",
    "        individual_val_returns = []\n",
    "        for _, row in val_trade_df.iterrows():\n",
    "            if row['pred'] == 1:\n",
    "                raw = row[f'Return{horizon}']\n",
    "            elif row['pred'] == 2:\n",
    "                raw = -row[f'Return{horizon}']\n",
    "            else:\n",
    "                continue\n",
    "            individual_val_returns.append(raw)\n",
    "\n",
    "        stop_loss_val = np.percentile(individual_val_returns, 5) if individual_val_returns else None\n",
    "\n",
    "        # Validate using TRAINING stop loss\n",
    "        val_returns = val_trade_df.groupby('DateTime').apply(\n",
    "            lambda x: calculate_grouped_returns(x, horizon, stop_loss_train)\n",
    "        ).values\n",
    "\n",
    "        total_val_return = (1 + val_returns).prod() - 1\n",
    "\n",
    "        if total_val_return > best_val_return:\n",
    "            best_val_return = total_val_return\n",
    "            best_horizon = horizon\n",
    "            best_model = model\n",
    "            best_stop_loss = stop_loss_val  # Save validation stop loss for test\n",
    "\n",
    "    # --- Test Best Model ---\n",
    "    if best_model and best_stop_loss is not None:\n",
    "        X_test = scaler.transform(test_data[feature_cols])\n",
    "        test_preds = best_model.predict(X_test)\n",
    "        test_probas = best_model.predict_proba(X_test)\n",
    "        y_test = test_data[f'Profit{best_horizon}'].values\n",
    "\n",
    "        test_trade_df = pd.DataFrame({\n",
    "            'DateTime': test_data['DateTime'],\n",
    "            'pred': test_preds,\n",
    "            'proba': [p[pred] for pred, p in zip(test_preds, test_probas)],\n",
    "            f'Return{best_horizon}': test_data[f'Return{best_horizon}']\n",
    "        })\n",
    "\n",
    "        grouped_returns = test_trade_df.groupby('DateTime').apply(\n",
    "            lambda x: calculate_grouped_returns(x, best_horizon, best_stop_loss)\n",
    "        ).values\n",
    "\n",
    "        all_test_returns.extend(grouped_returns)\n",
    "        all_dates.extend(test_trade_df['DateTime'].unique().tolist())\n",
    "        all_test_preds.extend(test_preds)\n",
    "        all_test_truths.extend(y_test)\n",
    "        all_test_probas.append(test_probas)\n",
    "        fold_mean_returns.append(np.mean(grouped_returns))\n",
    "        best_horizons_list.append(best_horizon or 'standard')\n",
    "\n",
    "    train_end_date += pd.DateOffset(years=1)\n",
    "\n",
    "# --- Results ---\n",
    "if all_test_returns:\n",
    "    cumulative_returns = (1 + np.array(all_test_returns)).cumprod() - 1\n",
    "    std_dev_individual = np.std(all_test_returns) * 100\n",
    "    min_return = np.nanmin(all_test_returns) * 100 if all_test_returns else 0.0\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(all_dates, cumulative_returns * 100)\n",
    "    plt.title('Cumulative Returns (DateTime-Based Rolling Windows)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cumulative Return (%)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Final Cumulative Compounded Return: {cumulative_returns[-1]*100:.2f}%\")\n",
    "    print(f\"\\nOverall Test Accuracy : {accuracy_score(all_test_truths, all_test_preds)*100:.2f}%\")\n",
    "    print(f\"Overall Precision     : {precision_score(all_test_truths, all_test_preds, average='macro', zero_division=0)*100:.2f}%\")\n",
    "    print(f\"Overall Recall        : {recall_score(all_test_truths, all_test_preds, average='macro', zero_division=0)*100:.2f}%\")\n",
    "    print(f\"Overall F1 Score      : {f1_score(all_test_truths, all_test_preds, average='macro', zero_division=0)*100:.2f}%\")\n",
    "    print(f\"Overall ROC AUC       : {roc_auc_score(all_test_truths, np.vstack(all_test_probas), multi_class='ovo', average='macro')*100:.2f}%\")\n",
    "    print(f\"\\nAverage Fold Test Return : {np.mean(fold_mean_returns)*100:.5f}%\")\n",
    "    print(f\"\\nStandard Deviation of All Test Returns (Aggregated Daily): {std_dev_individual:.5f}%\")\n",
    "    print(f\"\\nMinimum Return (Aggregated Daily): {min_return:.5f}%\")\n",
    "    print(\"\\nOptimal Horizon Counts:\")\n",
    "    print(pd.Series(best_horizons_list).value_counts())\n",
    "    print(\"\\nTest Set Position Counts:\")\n",
    "    counts = pd.Series(all_test_preds).value_counts().sort_index()\n",
    "    print(f\"Neutral (0): {counts.get(0, 0)}\")\n",
    "    print(f\"Long (1): {counts.get(1, 0)}\")\n",
    "    print(f\"Short (2): {counts.get(2, 0)}\")\n",
    "\n",
    "else:\n",
    "    print(\"No valid test periods found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a44fc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Add this after your existing print statements\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(all_test_truths, all_test_preds)\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                    index=['Actual Neutral (0)', 'Actual Long (1)', 'Actual Short (2)'],\n",
    "                    columns=['Pred Neutral (0)', 'Pred Long (1)', 'Pred Short (2)'])\n",
    "print(cm_df)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_test_truths, all_test_preds,\n",
    "                           target_names=['Neutral (0)', 'Long (1)', 'Short (2)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65e1424",
   "metadata": {},
   "source": [
    "<h1>Final Random Forest</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dbde45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['VECLIB_MAXIMUM_THREADS'] = '1'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# --- Configuration ---\n",
    "total_cost = 2 * transaction_cost\n",
    "horizons = ['', '_half', '_double']\n",
    "\n",
    "# --- Data Prep ---\n",
    "df_ml = df_ml.dropna(subset=['Surprise']).sort_values('DateTime')\n",
    "min_date = df_ml['DateTime'].min()\n",
    "max_date = df_ml['DateTime'].max()\n",
    "\n",
    "# --- Calculate Initial 30% Training Period ---\n",
    "total_duration = max_date - min_date\n",
    "initial_train_duration = total_duration * 0.3\n",
    "train_end_date = min_date + initial_train_duration\n",
    "\n",
    "# --- Tracking containers ---\n",
    "results = []\n",
    "all_test_returns = []\n",
    "all_dates = []\n",
    "all_test_preds = []\n",
    "all_test_truths = []\n",
    "all_test_probas = []\n",
    "fold_mean_returns = []\n",
    "best_horizons_list = []\n",
    "\n",
    "def calculate_grouped_returns(df_group, horizon, stop_loss=None):\n",
    "    \"\"\"Calculate returns with position sizing and stop loss\"\"\"\n",
    "    active_trades = df_group[df_group['pred'] != 0]\n",
    "    if active_trades.empty:\n",
    "        return 0.0\n",
    "\n",
    "    # Create copies with reset index\n",
    "    longs = active_trades[active_trades['pred'] == 1].copy().reset_index(drop=True)\n",
    "    shorts = active_trades[active_trades['pred'] == 2].copy().reset_index(drop=True)\n",
    "\n",
    "    total_confidence = longs['proba'].sum() + shorts['proba'].sum()\n",
    "    if total_confidence == 0:\n",
    "        return 0.0\n",
    "\n",
    "    num_trades = len(longs) + len(shorts)\n",
    "    total_costs = total_cost * num_trades\n",
    "\n",
    "    long_returns = 0.0\n",
    "    short_returns = 0.0\n",
    "\n",
    "    # Process longs with reset index\n",
    "    if not longs.empty:\n",
    "        long_weights = longs['proba'] / total_confidence\n",
    "        for idx in longs.index:\n",
    "            raw_return = longs.loc[idx, f'Return{horizon}']\n",
    "            capped_raw = max(stop_loss, raw_return) if stop_loss is not None else raw_return\n",
    "            long_returns += capped_raw * long_weights.loc[idx]\n",
    "\n",
    "    # Process shorts with reset index\n",
    "    if not shorts.empty:\n",
    "        short_weights = shorts['proba'] / total_confidence\n",
    "        for idx in shorts.index:\n",
    "            raw_return = -shorts.loc[idx, f'Return{horizon}']\n",
    "            capped_raw = max(stop_loss, raw_return) if stop_loss is not None else raw_return\n",
    "            short_returns += capped_raw * short_weights.loc[idx]\n",
    "\n",
    "    net_return = (long_returns + short_returns) - total_costs\n",
    "    return net_return\n",
    "\n",
    "# --- Main Loop ---\n",
    "while True:\n",
    "    val_start_date = train_end_date + pd.DateOffset(days=1)\n",
    "    val_end_date = val_start_date + pd.DateOffset(years=1)\n",
    "    test_start_date = val_end_date + pd.DateOffset(days=1)\n",
    "    test_end_date = test_start_date + pd.DateOffset(years=1)\n",
    "\n",
    "    if val_end_date > max_date or test_end_date > max_date:\n",
    "        break\n",
    "\n",
    "    train_mask = df_ml['DateTime'] <= train_end_date\n",
    "    val_mask = (df_ml['DateTime'] >= val_start_date) & (df_ml['DateTime'] <= val_end_date)\n",
    "    test_mask = (df_ml['DateTime'] >= test_start_date) & (df_ml['DateTime'] <= test_end_date)\n",
    "\n",
    "    train_data = df_ml[train_mask]\n",
    "    val_data = df_ml[val_mask]\n",
    "    test_data = df_ml[test_mask]\n",
    "\n",
    "    if val_data.empty or test_data.empty:\n",
    "        print(f\"Skipping period {val_start_date.date()} to {test_end_date.date()} (no data)\")\n",
    "        train_end_date += pd.DateOffset(years=1)\n",
    "        continue\n",
    "\n",
    "    # --- Horizon Optimization ---\n",
    "    best_horizon, best_model, best_stop_loss = None, None, None\n",
    "    best_val_return = -np.inf\n",
    "\n",
    "    for horizon in horizons:\n",
    "        # Feature scaling\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(train_data[feature_cols])\n",
    "        X_val = scaler.transform(val_data[feature_cols])\n",
    "        y_train = train_data[f'Profit{horizon}']\n",
    "        y_val = val_data[f'Profit{horizon}']\n",
    "\n",
    "        # Train model\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            criterion='gini',\n",
    "            min_samples_leaf=4,\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Get class probabilities\n",
    "        train_preds = model.predict(X_train)\n",
    "        train_probas = model.predict_proba(X_train)\n",
    "        class_map = {cls: idx for idx, cls in enumerate(model.classes_)}\n",
    "\n",
    "        # Training stop loss calculation\n",
    "        train_trade_df = pd.DataFrame({\n",
    "            'DateTime': train_data['DateTime'],\n",
    "            'pred': train_preds,\n",
    "            'proba': [proba[class_map[pred]] for pred, proba in zip(train_preds, train_probas)],\n",
    "            f'Return{horizon}': train_data[f'Return{horizon}']\n",
    "        })\n",
    "\n",
    "        train_raw_returns = []\n",
    "        for _, row in train_trade_df.iterrows():\n",
    "            if row['pred'] == 1:\n",
    "                train_raw_returns.append(row[f'Return{horizon}'])\n",
    "            elif row['pred'] == 2:\n",
    "                train_raw_returns.append(-row[f'Return{horizon}'])\n",
    "        stop_loss_train = np.percentile(train_raw_returns, 5) if train_raw_returns else None\n",
    "\n",
    "        # Validation predictions\n",
    "        val_preds = model.predict(X_val)\n",
    "        val_probas = model.predict_proba(X_val)\n",
    "        val_trade_df = pd.DataFrame({\n",
    "            'DateTime': val_data['DateTime'],\n",
    "            'pred': val_preds,\n",
    "            'proba': [proba[class_map[pred]] for pred, proba in zip(val_preds, val_probas)],\n",
    "            f'Return{horizon}': val_data[f'Return{horizon}']\n",
    "        })\n",
    "\n",
    "        # Validation stop loss calculation (for test set)\n",
    "        val_raw_returns = []\n",
    "        for _, row in val_trade_df.iterrows():\n",
    "            if row['pred'] == 1:\n",
    "                val_raw_returns.append(row[f'Return{horizon}'])\n",
    "            elif row['pred'] == 2:\n",
    "                val_raw_returns.append(-row[f'Return{horizon}'])\n",
    "        stop_loss_val = np.percentile(val_raw_returns, 5) if val_raw_returns else None\n",
    "\n",
    "        # Validate using training stop loss\n",
    "        val_returns = val_trade_df.groupby('DateTime').apply(\n",
    "            lambda x: calculate_grouped_returns(x, horizon, stop_loss_train)\n",
    "        ).values\n",
    "\n",
    "        total_val_return = (1 + val_returns).prod() - 1\n",
    "\n",
    "        if total_val_return > best_val_return:\n",
    "            best_val_return = total_val_return\n",
    "            best_horizon = horizon\n",
    "            best_model = model\n",
    "            best_stop_loss = stop_loss_val\n",
    "\n",
    "    # --- Test Best Model ---\n",
    "    if best_model and best_stop_loss is not None:\n",
    "        X_test = scaler.transform(test_data[feature_cols])\n",
    "        test_preds = best_model.predict(X_test)\n",
    "        test_probas = best_model.predict_proba(X_test)\n",
    "        y_test = test_data[f'Profit{best_horizon}'].values\n",
    "\n",
    "        class_map = {cls: idx for idx, cls in enumerate(best_model.classes_)}\n",
    "        test_trade_df = pd.DataFrame({\n",
    "            'DateTime': test_data['DateTime'],\n",
    "            'pred': test_preds,\n",
    "            'proba': [proba[class_map[pred]] for pred, proba in zip(test_preds, test_probas)],\n",
    "            f'Return{best_horizon}': test_data[f'Return{best_horizon}']\n",
    "        })\n",
    "\n",
    "        grouped_returns = test_trade_df.groupby('DateTime').apply(\n",
    "            lambda x: calculate_grouped_returns(x, best_horizon, best_stop_loss)\n",
    "        ).values\n",
    "\n",
    "        all_test_returns.extend(grouped_returns)\n",
    "        all_dates.extend(test_trade_df['DateTime'].unique().tolist())\n",
    "        all_test_preds.extend(test_preds)\n",
    "        all_test_truths.extend(y_test)\n",
    "        all_test_probas.append(test_probas)\n",
    "        fold_mean_returns.append(np.mean(grouped_returns))\n",
    "        best_horizons_list.append(best_horizon or 'standard')\n",
    "\n",
    "    train_end_date += pd.DateOffset(years=1)\n",
    "\n",
    "# --- Results ---\n",
    "if all_test_returns:\n",
    "    cumulative_returns = (1 + np.array(all_test_returns)).cumprod() - 1\n",
    "    std_dev_individual = np.std(all_test_returns) * 100\n",
    "    min_return = np.nanmin(all_test_returns) * 100 if all_test_returns else 0.0\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(all_dates, cumulative_returns * 100)\n",
    "    plt.title('Cumulative Returns (Random Forest with Stop Loss)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cumulative Return (%)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Final Cumulative Compounded Return: {cumulative_returns[-1]*100:.2f}%\")\n",
    "    print(f\"\\nOverall Test Accuracy : {accuracy_score(all_test_truths, all_test_preds)*100:.2f}%\")\n",
    "    print(f\"Overall Precision     : {precision_score(all_test_truths, all_test_preds, average='macro', zero_division=0)*100:.2f}%\")\n",
    "    print(f\"Overall Recall        : {recall_score(all_test_truths, all_test_preds, average='macro', zero_division=0)*100:.2f}%\")\n",
    "    print(f\"Overall F1 Score      : {f1_score(all_test_truths, all_test_preds, average='macro', zero_division=0)*100:.2f}%\")\n",
    "    print(f\"Overall ROC AUC       : {roc_auc_score(all_test_truths, np.vstack(all_test_probas), multi_class='ovo', average='macro')*100:.2f}%\")\n",
    "    print(f\"\\nAverage Fold Test Return : {np.mean(fold_mean_returns)*100:.5f}%\")\n",
    "    print(f\"\\nStandard Deviation of All Test Returns (Aggregated Daily): {std_dev_individual:.5f}%\")\n",
    "    print(f\"\\nMinimum Return (Aggregated Daily): {min_return:.5f}%\")\n",
    "    print(\"\\nOptimal Horizon Counts:\")\n",
    "    print(pd.Series(best_horizons_list).value_counts())\n",
    "    print(\"\\nTest Set Position Counts:\")\n",
    "    counts = pd.Series(all_test_preds).value_counts().sort_index()\n",
    "    print(f\"Neutral (0): {counts.get(0, 0)}\")\n",
    "    print(f\"Long (1): {counts.get(1, 0)}\")\n",
    "    print(f\"Short (2): {counts.get(2, 0)}\")\n",
    "\n",
    "else:\n",
    "    print(\"No valid test periods found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa39eed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Add this after your existing print statements\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(all_test_truths, all_test_preds)\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                    index=['Actual Neutral (0)', 'Actual Long (1)', 'Actual Short (2)'],\n",
    "                    columns=['Pred Neutral (0)', 'Pred Long (1)', 'Pred Short (2)'])\n",
    "print(cm_df)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_test_truths, all_test_preds,\n",
    "                           target_names=['Neutral (0)', 'Long (1)', 'Short (2)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4727e5a0",
   "metadata": {},
   "source": [
    "<h1>Final Logistic Regression</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd364abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --- Configuration ---\n",
    "total_cost = 2 * transaction_cost  # Total cost per trade (entry + exit)\n",
    "horizons = ['', '_half', '_double']\n",
    "\n",
    "# --- Data Prep ---\n",
    "df_ml = df_ml.dropna(subset=['Surprise']).sort_values('DateTime')\n",
    "min_date = df_ml['DateTime'].min()\n",
    "max_date = df_ml['DateTime'].max()\n",
    "\n",
    "# --- Calculate Initial 30% Training Period ---\n",
    "total_duration = max_date - min_date\n",
    "initial_train_duration = total_duration * 0.3\n",
    "train_end_date = min_date + initial_train_duration\n",
    "\n",
    "# --- Tracking containers ---\n",
    "all_test_returns = []\n",
    "all_dates = []\n",
    "all_test_preds = []\n",
    "all_test_truths = []\n",
    "all_test_probas = []\n",
    "fold_mean_returns = []\n",
    "best_horizons_list = []\n",
    "\n",
    "def calculate_grouped_returns(df_group, horizon, stop_loss=None):\n",
    "    \"\"\"Calculate returns with position sizing and stop loss\"\"\"\n",
    "    active_trades = df_group[df_group['pred'] != 0]\n",
    "    if active_trades.empty:\n",
    "        return 0.0\n",
    "\n",
    "    # Create copies with reset index\n",
    "    longs = active_trades[active_trades['pred'] == 1].copy().reset_index(drop=True)\n",
    "    shorts = active_trades[active_trades['pred'] == 2].copy().reset_index(drop=True)\n",
    "\n",
    "    total_confidence = longs['proba'].sum() + shorts['proba'].sum()\n",
    "    if total_confidence == 0:\n",
    "        return 0.0\n",
    "\n",
    "    num_trades = len(longs) + len(shorts)\n",
    "    total_costs = total_cost * num_trades\n",
    "\n",
    "    long_returns = 0.0\n",
    "    short_returns = 0.0\n",
    "\n",
    "    # Process longs with capping\n",
    "    if not longs.empty:\n",
    "        long_weights = longs['proba'] / total_confidence\n",
    "        for idx in longs.index:\n",
    "            raw_return = longs.loc[idx, f'Return{horizon}']\n",
    "            capped_raw = max(stop_loss, raw_return) if stop_loss is not None else raw_return\n",
    "            long_returns += capped_raw * long_weights.loc[idx]\n",
    "\n",
    "    # Process shorts with capping\n",
    "    if not shorts.empty:\n",
    "        short_weights = shorts['proba'] / total_confidence\n",
    "        for idx in shorts.index:\n",
    "            raw_return = -shorts.loc[idx, f'Return{horizon}']\n",
    "            capped_raw = max(stop_loss, raw_return) if stop_loss is not None else raw_return\n",
    "            short_returns += capped_raw * short_weights.loc[idx]\n",
    "\n",
    "    net_return = (long_returns + short_returns) - total_costs\n",
    "    return net_return\n",
    "\n",
    "# --- Main Loop ---\n",
    "while True:\n",
    "    val_start_date = train_end_date + pd.DateOffset(days=1)\n",
    "    val_end_date = val_start_date + pd.DateOffset(years=1)\n",
    "    test_start_date = val_end_date + pd.DateOffset(days=1)\n",
    "    test_end_date = test_start_date + pd.DateOffset(years=1)\n",
    "\n",
    "    if val_end_date > max_date or test_end_date > max_date:\n",
    "        break\n",
    "\n",
    "    train_mask = df_ml['DateTime'] <= train_end_date\n",
    "    val_mask = (df_ml['DateTime'] >= val_start_date) & (df_ml['DateTime'] <= val_end_date)\n",
    "    test_mask = (df_ml['DateTime'] >= test_start_date) & (df_ml['DateTime'] <= test_end_date)\n",
    "\n",
    "    train_data = df_ml[train_mask]\n",
    "    val_data = df_ml[val_mask]\n",
    "    test_data = df_ml[test_mask]\n",
    "\n",
    "    if val_data.empty or test_data.empty:\n",
    "        print(f\"Skipping period {val_start_date.date()} to {test_end_date.date()} (no data)\")\n",
    "        train_end_date += pd.DateOffset(years=1)\n",
    "        continue\n",
    "\n",
    "    # --- Horizon Optimization ---\n",
    "    best_horizon, best_model, best_stop_loss = None, None, None\n",
    "    best_val_return = -np.inf\n",
    "\n",
    "    for horizon in horizons:\n",
    "        # Feature scaling\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(train_data[feature_cols])\n",
    "        X_val = scaler.transform(val_data[feature_cols])\n",
    "        y_train = train_data[f'Profit{horizon}']\n",
    "        y_val = val_data[f'Profit{horizon}']\n",
    "\n",
    "        # Train Logistic Regression\n",
    "        model = LogisticRegression(\n",
    "            multi_class='multinomial',\n",
    "            solver='saga',\n",
    "            max_iter=100,\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Training stop loss calculation\n",
    "        train_preds = model.predict(X_train)\n",
    "        train_probas = model.predict_proba(X_train)\n",
    "        class_map = {cls: idx for idx, cls in enumerate(model.classes_)}\n",
    "\n",
    "        train_trade_df = pd.DataFrame({\n",
    "            'DateTime': train_data['DateTime'],\n",
    "            'pred': train_preds,\n",
    "            'proba': [proba[class_map[pred]] for pred, proba in zip(train_preds, train_probas)],\n",
    "            f'Return{horizon}': train_data[f'Return{horizon}']\n",
    "        })\n",
    "\n",
    "        train_raw_returns = []\n",
    "        for _, row in train_trade_df.iterrows():\n",
    "            if row['pred'] == 1:\n",
    "                train_raw_returns.append(row[f'Return{horizon}'])\n",
    "            elif row['pred'] == 2:\n",
    "                train_raw_returns.append(-row[f'Return{horizon}'])\n",
    "        stop_loss_train = np.percentile(train_raw_returns, 5) if train_raw_returns else None\n",
    "\n",
    "        # Validation predictions and stop loss\n",
    "        val_preds = model.predict(X_val)\n",
    "        val_probas = model.predict_proba(X_val)\n",
    "        val_trade_df = pd.DataFrame({\n",
    "            'DateTime': val_data['DateTime'],\n",
    "            'pred': val_preds,\n",
    "            'proba': [proba[class_map[pred]] for pred, proba in zip(val_preds, val_probas)],\n",
    "            f'Return{horizon}': val_data[f'Return{horizon}']\n",
    "        })\n",
    "\n",
    "        val_raw_returns = []\n",
    "        for _, row in val_trade_df.iterrows():\n",
    "            if row['pred'] == 1:\n",
    "                val_raw_returns.append(row[f'Return{horizon}'])\n",
    "            elif row['pred'] == 2:\n",
    "                val_raw_returns.append(-row[f'Return{horizon}'])\n",
    "        stop_loss_val = np.percentile(val_raw_returns, 5) if val_raw_returns else None\n",
    "\n",
    "        # Validate using training stop loss\n",
    "        val_returns = val_trade_df.groupby('DateTime').apply(\n",
    "            lambda x: calculate_grouped_returns(x, horizon, stop_loss_train)\n",
    "        ).values\n",
    "\n",
    "        total_val_return = (1 + val_returns).prod() - 1\n",
    "\n",
    "        if total_val_return > best_val_return:\n",
    "            best_val_return = total_val_return\n",
    "            best_horizon = horizon\n",
    "            best_model = model\n",
    "            best_stop_loss = stop_loss_val\n",
    "\n",
    "    # --- Test Best Model ---\n",
    "    if best_model and best_stop_loss is not None:\n",
    "        X_test = scaler.transform(test_data[feature_cols])\n",
    "        test_preds = best_model.predict(X_test)\n",
    "        test_probas = best_model.predict_proba(X_test)\n",
    "        y_test = test_data[f'Profit{best_horizon}'].values\n",
    "\n",
    "        class_map = {cls: idx for idx, cls in enumerate(best_model.classes_)}\n",
    "        test_trade_df = pd.DataFrame({\n",
    "            'DateTime': test_data['DateTime'],\n",
    "            'pred': test_preds,\n",
    "            'proba': [proba[class_map[pred]] for pred, proba in zip(test_preds, test_probas)],\n",
    "            f'Return{best_horizon}': test_data[f'Return{best_horizon}']\n",
    "        })\n",
    "\n",
    "        grouped_returns = test_trade_df.groupby('DateTime').apply(\n",
    "            lambda x: calculate_grouped_returns(x, best_horizon, best_stop_loss)\n",
    "        ).values\n",
    "\n",
    "        all_test_returns.extend(grouped_returns)\n",
    "        all_dates.extend(test_trade_df['DateTime'].unique().tolist())\n",
    "        all_test_preds.extend(test_preds)\n",
    "        all_test_truths.extend(y_test)\n",
    "        all_test_probas.append(test_probas)\n",
    "        fold_mean_returns.append(np.mean(grouped_returns))\n",
    "        best_horizons_list.append(best_horizon or 'standard')\n",
    "\n",
    "    train_end_date += pd.DateOffset(years=1)\n",
    "\n",
    "# --- Results ---\n",
    "if all_test_returns:\n",
    "    cumulative_returns = (1 + np.array(all_test_returns)).cumprod() - 1\n",
    "    std_dev_individual = np.std(all_test_returns) * 100\n",
    "    min_return = np.nanmin(all_test_returns) * 100 if all_test_returns else 0.0\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(all_dates, cumulative_returns * 100)\n",
    "    plt.title('Cumulative Returns (Logistic Regression with Stop Loss)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cumulative Return (%)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Final Cumulative Compounded Return: {cumulative_returns[-1]*100:.2f}%\")\n",
    "    print(f\"\\nOverall Test Accuracy : {accuracy_score(all_test_truths, all_test_preds)*100:.2f}%\")\n",
    "    print(f\"Overall Precision     : {precision_score(all_test_truths, all_test_preds, average='macro', zero_division=0)*100:.2f}%\")\n",
    "    print(f\"Overall Recall        : {recall_score(all_test_truths, all_test_preds, average='macro', zero_division=0)*100:.2f}%\")\n",
    "    print(f\"Overall F1 Score      : {f1_score(all_test_truths, all_test_preds, average='macro', zero_division=0)*100:.2f}%\")\n",
    "    print(f\"Overall ROC AUC       : {roc_auc_score(all_test_truths, np.vstack(all_test_probas), multi_class='ovo', average='macro')*100:.2f}%\")\n",
    "    print(f\"\\nAverage Fold Test Return : {np.mean(fold_mean_returns)*100:.5f}%\")\n",
    "    print(f\"\\nStandard Deviation of All Test Returns (Aggregated Daily): {std_dev_individual:.5f}%\")\n",
    "    print(f\"\\nMinimum Return (Aggregated Daily): {min_return:.5f}%\")\n",
    "    print(\"\\nOptimal Horizon Counts:\")\n",
    "    print(pd.Series(best_horizons_list).value_counts())\n",
    "    print(\"\\nTest Set Position Counts:\")\n",
    "    counts = pd.Series(all_test_preds).value_counts().sort_index()\n",
    "    print(f\"Neutral (0): {counts.get(0, 0)}\")\n",
    "    print(f\"Long (1): {counts.get(1, 0)}\")\n",
    "    print(f\"Short (2): {counts.get(2, 0)}\")\n",
    "\n",
    "else:\n",
    "    print(\"No valid test periods found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4812d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Add this after your existing print statements\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(all_test_truths, all_test_preds)\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                    index=['Actual Neutral (0)', 'Actual Long (1)', 'Actual Short (2)'],\n",
    "                    columns=['Pred Neutral (0)', 'Pred Long (1)', 'Pred Short (2)'])\n",
    "print(cm_df)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_test_truths, all_test_preds,\n",
    "                           target_names=['Neutral (0)', 'Long (1)', 'Short (2)']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
