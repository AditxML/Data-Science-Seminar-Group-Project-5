{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3291d80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_24808\\3191809411.py:11: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_surprise = pd.read_csv('US_economic_releases_events.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data and drop unnecessary columns\n",
    "# get all ES_part_X files, 1 up to 5 and place in one df_es dataframe\n",
    "df_es = pd.DataFrame()\n",
    "for i in range(1, 6):\n",
    "    df_temp = pd.read_csv(f'ES_part_{i}.csv')\n",
    "    df_es = pd.concat([df_es, df_temp], ignore_index=True)\n",
    "\n",
    "df_surprise = pd.read_csv('US_economic_releases_events.csv')\n",
    "df_surprise.drop(columns=['S', 'Month', 'Surv(A)', 'Surv(H)', 'Surv(L)', 'Flag', 'Country/Region', 'Day', 'C', 'Category', 'Subcategory', 'Std Dev', 'Period', 'Actual'], inplace=True)\n",
    "\n",
    "\n",
    "# Clean and preprocess surprise data\n",
    "df_surprise.dropna(subset=['Surprise'], inplace=True)\n",
    "df_surprise = df_surprise[df_surprise['Surprise'] != 0]\n",
    "df_surprise.replace(\"--\", pd.NA, inplace=True)\n",
    "df_surprise.dropna(subset=['Surprise'], inplace=True)\n",
    "df_surprise = df_surprise[df_surprise['Surprise'] != 0]\n",
    "df_surprise['Surprise'] = pd.to_numeric(df_surprise['Surprise'], errors='coerce')\n",
    "df_surprise = df_surprise[df_surprise['Surprise'] != 0].dropna(subset=['Surprise'])\n",
    "df_surprise.dropna(subset=['Time'], inplace=True)\n",
    "lower_bound = df_surprise['Surprise'].quantile(0.005)\n",
    "upper_bound = df_surprise['Surprise'].quantile(0.995)\n",
    "df_surprise = df_surprise[(df_surprise['Surprise'] >= lower_bound) & (df_surprise['Surprise'] <= upper_bound)]\n",
    "\n",
    "# Create DateTime columns\n",
    "df_surprise['Date'] = df_surprise['Unnamed: 0'].astype(str)\n",
    "df_surprise['Time'] = df_surprise['Time'].astype(str)\n",
    "df_surprise['DateTime'] = pd.to_datetime(df_surprise['Date'].str[:10] + ' ' + df_surprise['Time'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "df_surprise.drop(columns=['Unnamed: 0','Date'], inplace=True)\n",
    "\n",
    "df_es['Date'] = df_es['Date'].astype(str)\n",
    "df_es['Time'] = df_es['Time'].astype(str) + ':00'\n",
    "df_es['DateTime'] = pd.to_datetime(df_es['Date'] + ' ' + df_es['Time'], format='%m/%d/%Y %H:%M:%S', errors='coerce')\n",
    "\n",
    "# Merge dataframes\n",
    "df_combined = pd.merge(df_es, df_surprise, on='DateTime', how='outer', suffixes=('_es', '_surprise'), indicator=True)\n",
    "df_combined.dropna(subset=['Open'], inplace=True)\n",
    "\n",
    "df_combined\n",
    "\n",
    "# Sort df_combined by DateTime\n",
    "df_combined.sort_values(by='DateTime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "530e22fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Time_es', 'Open', 'Close', 'Volume', 'DateTime', 'Event',\n",
       "       'Ticker', 'Prior', 'Revised', 'Freq.', 'First Rev.', 'Last Rev.',\n",
       "       'Time_surprise', 'R', 'Surv(M)', '# Ests.', 'Surprise', '_merge'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out all columns from df combined\n",
    "df_combined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51fb8f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time_es</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Event</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Prior</th>\n",
       "      <th>Revised</th>\n",
       "      <th>Freq.</th>\n",
       "      <th>First Rev.</th>\n",
       "      <th>Last Rev.</th>\n",
       "      <th>Time_surprise</th>\n",
       "      <th>R</th>\n",
       "      <th>Surv(M)</th>\n",
       "      <th># Ests.</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>09/10/1997</td>\n",
       "      <td>00:01:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1997-09-10 00:01:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>09/10/1997</td>\n",
       "      <td>00:02:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1997-09-10 00:02:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>09/10/1997</td>\n",
       "      <td>00:03:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1997-09-10 00:03:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>09/10/1997</td>\n",
       "      <td>00:04:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1997-09-10 00:04:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>09/10/1997</td>\n",
       "      <td>00:05:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1997-09-10 00:05:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9695761</th>\n",
       "      <td>12/19/2024</td>\n",
       "      <td>15:56:00</td>\n",
       "      <td>5941.75</td>\n",
       "      <td>5941.75</td>\n",
       "      <td>318.0</td>\n",
       "      <td>2024-12-19 15:56:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9695762</th>\n",
       "      <td>12/19/2024</td>\n",
       "      <td>15:57:00</td>\n",
       "      <td>5941.75</td>\n",
       "      <td>5941.50</td>\n",
       "      <td>386.0</td>\n",
       "      <td>2024-12-19 15:57:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9695763</th>\n",
       "      <td>12/19/2024</td>\n",
       "      <td>15:58:00</td>\n",
       "      <td>5941.50</td>\n",
       "      <td>5941.00</td>\n",
       "      <td>484.0</td>\n",
       "      <td>2024-12-19 15:58:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9695764</th>\n",
       "      <td>12/19/2024</td>\n",
       "      <td>15:59:00</td>\n",
       "      <td>5940.75</td>\n",
       "      <td>5941.00</td>\n",
       "      <td>6462.0</td>\n",
       "      <td>2024-12-19 15:59:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9695765</th>\n",
       "      <td>12/19/2024</td>\n",
       "      <td>16:00:00</td>\n",
       "      <td>5941.00</td>\n",
       "      <td>5941.25</td>\n",
       "      <td>8864.0</td>\n",
       "      <td>2024-12-19 16:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9695665 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Date   Time_es     Open    Close  Volume            DateTime  \\\n",
       "2        09/10/1997  00:01:00     0.00     0.00     0.0 1997-09-10 00:01:00   \n",
       "3        09/10/1997  00:02:00     0.00     0.00     0.0 1997-09-10 00:02:00   \n",
       "4        09/10/1997  00:03:00     0.00     0.00     0.0 1997-09-10 00:03:00   \n",
       "5        09/10/1997  00:04:00     0.00     0.00     0.0 1997-09-10 00:04:00   \n",
       "6        09/10/1997  00:05:00     0.00     0.00     0.0 1997-09-10 00:05:00   \n",
       "...             ...       ...      ...      ...     ...                 ...   \n",
       "9695761  12/19/2024  15:56:00  5941.75  5941.75   318.0 2024-12-19 15:56:00   \n",
       "9695762  12/19/2024  15:57:00  5941.75  5941.50   386.0 2024-12-19 15:57:00   \n",
       "9695763  12/19/2024  15:58:00  5941.50  5941.00   484.0 2024-12-19 15:58:00   \n",
       "9695764  12/19/2024  15:59:00  5940.75  5941.00  6462.0 2024-12-19 15:59:00   \n",
       "9695765  12/19/2024  16:00:00  5941.00  5941.25  8864.0 2024-12-19 16:00:00   \n",
       "\n",
       "        Event Ticker Prior Revised Freq. First Rev. Last Rev. Time_surprise  \\\n",
       "2         NaN    NaN   NaN     NaN   NaN        NaN       NaN           NaN   \n",
       "3         NaN    NaN   NaN     NaN   NaN        NaN       NaN           NaN   \n",
       "4         NaN    NaN   NaN     NaN   NaN        NaN       NaN           NaN   \n",
       "5         NaN    NaN   NaN     NaN   NaN        NaN       NaN           NaN   \n",
       "6         NaN    NaN   NaN     NaN   NaN        NaN       NaN           NaN   \n",
       "...       ...    ...   ...     ...   ...        ...       ...           ...   \n",
       "9695761   NaN    NaN   NaN     NaN   NaN        NaN       NaN           NaN   \n",
       "9695762   NaN    NaN   NaN     NaN   NaN        NaN       NaN           NaN   \n",
       "9695763   NaN    NaN   NaN     NaN   NaN        NaN       NaN           NaN   \n",
       "9695764   NaN    NaN   NaN     NaN   NaN        NaN       NaN           NaN   \n",
       "9695765   NaN    NaN   NaN     NaN   NaN        NaN       NaN           NaN   \n",
       "\n",
       "          R Surv(M)  # Ests.  Surprise     _merge  \n",
       "2       NaN     NaN      NaN       NaN  left_only  \n",
       "3       NaN     NaN      NaN       NaN  left_only  \n",
       "4       NaN     NaN      NaN       NaN  left_only  \n",
       "5       NaN     NaN      NaN       NaN  left_only  \n",
       "6       NaN     NaN      NaN       NaN  left_only  \n",
       "...      ..     ...      ...       ...        ...  \n",
       "9695761 NaN     NaN      NaN       NaN  left_only  \n",
       "9695762 NaN     NaN      NaN       NaN  left_only  \n",
       "9695763 NaN     NaN      NaN       NaN  left_only  \n",
       "9695764 NaN     NaN      NaN       NaN  left_only  \n",
       "9695765 NaN     NaN      NaN       NaN  left_only  \n",
       "\n",
       "[9695665 rows x 19 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0d31bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Close</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8726186</th>\n",
       "      <td>2022-03-30 04:59:00</td>\n",
       "      <td>5101.44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8726187</th>\n",
       "      <td>2022-03-30 05:00:00</td>\n",
       "      <td>5102.83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8726188</th>\n",
       "      <td>2022-03-30 05:01:00</td>\n",
       "      <td>5098.40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8726189</th>\n",
       "      <td>2022-03-30 05:02:00</td>\n",
       "      <td>5099.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8726190</th>\n",
       "      <td>2022-03-30 05:03:00</td>\n",
       "      <td>5100.06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9695761</th>\n",
       "      <td>2024-12-19 15:56:00</td>\n",
       "      <td>5941.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9695762</th>\n",
       "      <td>2024-12-19 15:57:00</td>\n",
       "      <td>5941.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9695763</th>\n",
       "      <td>2024-12-19 15:58:00</td>\n",
       "      <td>5941.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9695764</th>\n",
       "      <td>2024-12-19 15:59:00</td>\n",
       "      <td>5941.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9695765</th>\n",
       "      <td>2024-12-19 16:00:00</td>\n",
       "      <td>5941.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>969567 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   DateTime    Close  Surprise Ticker Event\n",
       "8726186 2022-03-30 04:59:00  5101.44       NaN    NaN   NaN\n",
       "8726187 2022-03-30 05:00:00  5102.83       NaN    NaN   NaN\n",
       "8726188 2022-03-30 05:01:00  5098.40       NaN    NaN   NaN\n",
       "8726189 2022-03-30 05:02:00  5099.78       NaN    NaN   NaN\n",
       "8726190 2022-03-30 05:03:00  5100.06       NaN    NaN   NaN\n",
       "...                     ...      ...       ...    ...   ...\n",
       "9695761 2024-12-19 15:56:00  5941.75       NaN    NaN   NaN\n",
       "9695762 2024-12-19 15:57:00  5941.50       NaN    NaN   NaN\n",
       "9695763 2024-12-19 15:58:00  5941.00       NaN    NaN   NaN\n",
       "9695764 2024-12-19 15:59:00  5941.00       NaN    NaN   NaN\n",
       "9695765 2024-12-19 16:00:00  5941.25       NaN    NaN   NaN\n",
       "\n",
       "[969567 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only keep the following columns: 'DateTime', 'Close', 'Surprise', 'Ticker', 'Event'\n",
    "df_combined = df_combined[['DateTime', 'Close', 'Surprise', 'Ticker', 'Event']]\n",
    "\n",
    "# Save the last 10% of the data into a csv file, Dont use random, just take the first 10% of rows)\n",
    "df_combined.iloc[int(len(df_combined)*0.9):].to_csv('last_10_percent.csv', index=False)\n",
    "\n",
    "# print out the last 10% of the data\n",
    "df_combined.iloc[int(len(df_combined)*0.9):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce99f5f",
   "metadata": {},
   "source": [
    "<h1>Machine Learning Model</h1>\n",
    "<p>Frist creating classification variables using TC of 0.0025bps and explanatory variables to use in models</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "756070e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from tensorflow) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from tensorflow) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from tensorflow) (3.9.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jop brouwer\\documents\\github\\data-science-seminar-group-project-5\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy scikit-learn matplotlib tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae97539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from last_10_percent.csv...\n",
      "Data loaded successfully.\n",
      "Starting preprocessing...\n",
      "DateTime converted.\n",
      "Removed 2155 rows with duplicate DateTime entries.\n",
      "DateTime index set and sorted.\n",
      "Found 1094 surprise events.\n",
      "Calculating future returns...\n",
      "Attempting reindex on df_full (size: 967412, index unique: True) using 1094 target times.\n",
      "Reindex successful.\n",
      "Calculated future returns for 1094 events.\n",
      "Target signals defined.\n",
      "Signal distribution (Actual):\n",
      " signal\n",
      " 1    0.372943\n",
      "-1    0.364717\n",
      " 0    0.262340\n",
      "Name: proportion, dtype: float64\n",
      "Filled potential NaNs in 'Event' column.\n",
      "\n",
      "Skipping model building and training due to lack of processed data or preprocessing failure.\n",
      "\n",
      "Skipping model evaluation as the model was not trained or test data is unavailable.\n",
      "\n",
      "Skipping backtesting due to lack of model predictions or necessary info.\n",
      "\n",
      "--- Final Status ---\n",
      "Process Halted: Model training did not complete successfully.\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import traceback # Import traceback for detailed error printing\n",
    "\n",
    "# Ensure a clean TensorFlow state (important if running multiple times in the same environment)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# --- Configuration ---\n",
    "FILENAME = 'last_10_percent.csv' # Make sure this file is in the same directory or provide the full path\n",
    "TRADING_WINDOW = 20 # minutes to hold the trade\n",
    "PRICE_CHANGE_THRESHOLD = 0.0005 # 0.05% threshold for buy/sell signal definition\n",
    "TEST_SIZE = 0.2 # Use last 20% of surprise events for testing\n",
    "N_CLASSES = 3 # Long (1), Hold (0), Short (-1) -> mapped to 0, 1, 2 for Keras\n",
    "VALIDATION_SPLIT = 0.2 # Use 20% of training data for validation during training\n",
    "EPOCHS = 50 # Max number of epochs to train\n",
    "BATCH_SIZE = 32 # Number of samples per gradient update\n",
    "EARLY_STOPPING_PATIENCE = 5 # Stop training if validation loss doesn't improve for 5 epochs\n",
    "\n",
    "# --- Initialize Variables ---\n",
    "# Ensure all key variables that control workflow are initialized to None\n",
    "df = None\n",
    "df_surprise = None\n",
    "X_train_processed, y_train = None, None\n",
    "X_test_processed, y_test = None, None\n",
    "preprocessing_info = None\n",
    "preprocessor = None\n",
    "model = None\n",
    "history = None\n",
    "trade_results = None\n",
    "predicted_signals = None\n",
    "\n",
    "# --- Load Data ---\n",
    "print(f\"Loading data from {FILENAME}...\")\n",
    "try:\n",
    "    df = pd.read_csv(FILENAME)\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{FILENAME}' not found. Please place it in the correct directory.\")\n",
    "    # df remains None\n",
    "\n",
    "# --- Data Preprocessing ---\n",
    "if df is not None:\n",
    "    print(\"Starting preprocessing...\")\n",
    "    # 1. Convert DateTime and Handle Duplicates\n",
    "    try:\n",
    "        # Check if 'DateTime' column exists before using it\n",
    "        if 'DateTime' not in df.columns:\n",
    "            raise KeyError(\"'DateTime' column not found in the CSV file.\")\n",
    "\n",
    "        df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "        print(\"DateTime converted.\")\n",
    "\n",
    "        # Handle Duplicate Timestamps\n",
    "        n_original = len(df)\n",
    "        df = df.drop_duplicates(subset=['DateTime'], keep='first')\n",
    "        n_after_drop = len(df)\n",
    "        if n_original > n_after_drop:\n",
    "            print(f\"Removed {n_original - n_after_drop} rows with duplicate DateTime entries.\")\n",
    "\n",
    "        # 2. Set Index and Sort\n",
    "        df = df.set_index('DateTime')\n",
    "        df = df.sort_index()\n",
    "        print(\"DateTime index set and sorted.\")\n",
    "\n",
    "    except KeyError as ke:\n",
    "        print(f\"Error: {ke}\")\n",
    "        df = None # Stop processing if essential column missing\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing DateTime column: {e}\")\n",
    "        df = None\n",
    "\n",
    "# Check for essential columns after basic processing\n",
    "essential_cols = ['Close', 'Surprise', 'Event']\n",
    "if df is not None and all(col in df.columns for col in essential_cols):\n",
    "\n",
    "    # Keep necessary columns - df now has a unique index\n",
    "    df_full = df.copy() # Keep a copy for price lookups\n",
    "    df = df[['Close', 'Surprise', 'Event']].copy() # Work with relevant columns\n",
    "\n",
    "    # 3. Filter for surprise events\n",
    "    df_surprise = df.dropna(subset=['Surprise']).copy()\n",
    "    print(f\"Found {len(df_surprise)} initial surprise events.\")\n",
    "\n",
    "    if len(df_surprise) >= 50: # Need some data to proceed\n",
    "        # 4. Define Target Variable (Future Return and Signal)\n",
    "        print(\"Calculating future returns...\")\n",
    "        df_surprise['future_price'] = np.nan\n",
    "        target_times = df_surprise.index + pd.Timedelta(minutes=TRADING_WINDOW)\n",
    "\n",
    "        # Ensure we only look up times within the original data range\n",
    "        valid_target_times = target_times[target_times <= df_full.index.max()]\n",
    "        valid_surprise_indices = df_surprise.index[target_times <= df_full.index.max()]\n",
    "\n",
    "        if not valid_surprise_indices.empty:\n",
    "            # The reindex operation should now work as df_full has a unique index\n",
    "            print(f\"Attempting reindex on df_full (size: {len(df_full)}, index unique: {df_full.index.is_unique}) using {len(valid_target_times)} target times.\")\n",
    "            try:\n",
    "                future_prices = df_full['Close'].reindex(valid_target_times, method='ffill')\n",
    "                future_prices.index = valid_surprise_indices # Align index back to the surprise time\n",
    "                df_surprise.loc[valid_surprise_indices, 'future_price'] = future_prices\n",
    "                print(\"Reindex successful.\")\n",
    "            except ValueError as ve:\n",
    "                 print(f\"ERROR during reindex even after dropping duplicates: {ve}\")\n",
    "                 df_surprise = None # Stop processing on error\n",
    "\n",
    "            if df_surprise is not None: # Check if reindex succeeded\n",
    "                # Calculate future return where future price is available\n",
    "                df_surprise['future_return'] = (df_surprise['future_price'] - df_surprise['Close']) / df_surprise['Close']\n",
    "\n",
    "                # Drop rows where future return couldn't be calculated\n",
    "                initial_count = len(df_surprise)\n",
    "                df_surprise = df_surprise.dropna(subset=['future_return'])\n",
    "                dropped_count = initial_count - len(df_surprise)\n",
    "                if dropped_count > 0:\n",
    "                    print(f\"Dropped {dropped_count} events where future return could not be calculated (e.g., near end of data).\")\n",
    "                print(f\"Number of events with valid future returns: {len(df_surprise)}\")\n",
    "\n",
    "                # --- Start of block for splitting and preprocessing ---\n",
    "                # This block needs df_surprise to have enough data\n",
    "                if len(df_surprise) >= 50:\n",
    "                    try:\n",
    "                        # Define signal based on threshold\n",
    "                        df_surprise['signal'] = 0 # Default to Hold\n",
    "                        df_surprise.loc[df_surprise['future_return'] > PRICE_CHANGE_THRESHOLD, 'signal'] = 1 # Long\n",
    "                        df_surprise.loc[df_surprise['future_return'] < -PRICE_CHANGE_THRESHOLD, 'signal'] = -1 # Short\n",
    "\n",
    "                        # Map signals for Keras: Short: 0, Hold: 1, Long: 2\n",
    "                        signal_mapping = {-1: 0, 0: 1, 1: 2}\n",
    "                        df_surprise['target_keras'] = df_surprise['signal'].map(signal_mapping)\n",
    "                        print(\"Target signals defined.\")\n",
    "                        print(\"Signal distribution (Actual):\\n\", df_surprise['signal'].value_counts(normalize=True))\n",
    "\n",
    "                        # Fill any remaining NaNs in 'Event' column\n",
    "                        df_surprise['Event'] = df_surprise['Event'].fillna('Unknown_Event')\n",
    "                        print(\"Filled potential NaNs in 'Event' column.\")\n",
    "\n",
    "                        # 5. Prepare Features (X) and Target (y)\n",
    "                        print(\"Preparing features (X) and target (y)...\")\n",
    "                        features = ['Surprise', 'Event']\n",
    "                        target = 'target_keras'\n",
    "                        X = df_surprise[features]\n",
    "                        y = df_surprise[target]\n",
    "                        # Convert target to categorical format for Keras\n",
    "                        y_cat = to_categorical(y, num_classes=N_CLASSES)\n",
    "                        print(\"Features and target prepared.\")\n",
    "\n",
    "                        # 6. Split Data Chronologically\n",
    "                        print(\"Splitting data...\")\n",
    "                        split_index = int(len(X) * (1 - TEST_SIZE))\n",
    "                        if split_index <= 0 or split_index >= len(X): # Ensure both sets have data\n",
    "                             raise ValueError(f\"Test size ({TEST_SIZE}) results in an empty train or test set (Split index: {split_index}, Total size: {len(X)}).\")\n",
    "\n",
    "                        X_train, X_test = X[:split_index], X[split_index:]\n",
    "                        y_train_cat, y_test_cat = y_cat[:split_index], y_cat[split_index:] # Use different names for clarity\n",
    "                        signals_test = df_surprise['signal'][split_index:]\n",
    "                        returns_test = df_surprise['future_return'][split_index:]\n",
    "                        dates_test = df_surprise.index[split_index:]\n",
    "                        print(f\"Data split: {len(X_train)} train, {len(X_test)} test samples.\")\n",
    "                        # Check if split resulted in empty dataframes which might cause issues later\n",
    "                        if X_train.empty or X_test.empty:\n",
    "                            raise ValueError(\"Train or test split resulted in empty DataFrame.\")\n",
    "\n",
    "                        # Assign y_train here AFTER successful split\n",
    "                        y_train = y_train_cat\n",
    "                        y_test = y_test_cat\n",
    "\n",
    "\n",
    "                        # 7. Preprocessing Pipeline\n",
    "                        print(\"Defining preprocessing pipeline...\")\n",
    "                        numerical_features = ['Surprise']\n",
    "                        categorical_features = ['Event']\n",
    "                        numerical_transformer = StandardScaler()\n",
    "                        categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "                        preprocessor = ColumnTransformer(\n",
    "                            transformers=[\n",
    "                                ('num', numerical_transformer, numerical_features),\n",
    "                                ('cat', categorical_transformer, categorical_features)\n",
    "                            ],\n",
    "                            remainder='passthrough'\n",
    "                            )\n",
    "                        print(\"Pipeline defined.\")\n",
    "\n",
    "                        # Apply preprocessing\n",
    "                        print(\"Applying preprocessing (fit_transform on train)...\")\n",
    "                        X_train_processed = preprocessor.fit_transform(X_train) # X_train_processed assigned here\n",
    "                        print(f\"Shape of processed training features: {X_train_processed.shape}\")\n",
    "                        print(\"Applying preprocessing (transform on test)...\")\n",
    "                        X_test_processed = preprocessor.transform(X_test) # X_test_processed assigned here\n",
    "                        print(f\"Shape of processed testing features: {X_test_processed.shape}\")\n",
    "                        n_features = X_train_processed.shape[1]\n",
    "                        print(f\"Preprocessing complete. Number of features: {n_features}\")\n",
    "\n",
    "                        # Store preprocessor and related info for later use\n",
    "                        print(\"Storing preprocessing info...\")\n",
    "                        preprocessing_info = {\n",
    "                            'n_features': n_features,\n",
    "                            'dates_test': dates_test,\n",
    "                            'signals_test': signals_test,\n",
    "                            'returns_test': returns_test,\n",
    "                            'signal_mapping_inv': {v: k for k, v in signal_mapping.items()}\n",
    "                        }\n",
    "                        print(\"Preprocessing info stored.\")\n",
    "                        # --- End of successful splitting/preprocessing block ---\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"ERROR during data preparation/splitting/preprocessing: {e}\")\n",
    "                        # Ensure variables remain None or are reset if error occurs mid-process\n",
    "                        X_train_processed = None\n",
    "                        y_train = None # Reset y_train as well\n",
    "                        X_test_processed = None\n",
    "                        y_test = None\n",
    "                        preprocessing_info = None\n",
    "                        print(\"Variables reset due to error.\")\n",
    "                        traceback.print_exc() # Print detailed traceback\n",
    "\n",
    "                # --- End of block requiring >= 50 events ---\n",
    "                else:\n",
    "                     print(\"Error: Not enough data remaining after calculating future returns (<50 events).\")\n",
    "                     df_surprise = None # Flag failure\n",
    "\n",
    "        else:\n",
    "            print(\"Error: Could not determine valid future time targets (check data range and TRADING_WINDOW).\")\n",
    "            df_surprise = None # Flag failure\n",
    "    elif df is not None:\n",
    "         # Handling missing essential columns after initial load/processing check\n",
    "         missing_cols = [col for col in essential_cols if col not in df.columns]\n",
    "         if missing_cols:\n",
    "             print(f\"Error: Missing essential columns after initial processing: {missing_cols}\")\n",
    "         else:\n",
    "             # This case handles df being too small initially\n",
    "             print(\"Error: Not enough data in the initial DataFrame to proceed.\")\n",
    "         df_surprise = None # Flag failure\n",
    "    else:\n",
    "         # df was None from the start or after DateTime processing error\n",
    "         print(\"Error: Data loading or initial processing failed.\")\n",
    "         df_surprise = None # df_surprise should already be None\n",
    "\n",
    "\n",
    "# --- Build and Train Neural Network ---\n",
    "# Check if ALL necessary inputs for training are valid\n",
    "if X_train_processed is not None and y_train is not None and preprocessing_info is not None:\n",
    "    print(f\"\\nProceeding to build model with {preprocessing_info['n_features']} input features...\")\n",
    "\n",
    "    # Define the model architecture\n",
    "    model = Sequential(name=\"Macro_Surprise_Trader\")\n",
    "    model.add(Input(shape=(preprocessing_info['n_features'],), name='Input_Layer'))\n",
    "    model.add(Dense(64, activation='relu', name='Hidden_Layer_1'))\n",
    "    model.add(Dropout(0.25, name='Dropout_1')) # Regularization\n",
    "    model.add(Dense(32, activation='relu', name='Hidden_Layer_2'))\n",
    "    model.add(Dropout(0.25, name='Dropout_2')) # Regularization\n",
    "    model.add(Dense(N_CLASSES, activation='softmax', name='Output_Layer')) # Output layer for 3 classes\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy', # Suitable for multi-class classification\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "\n",
    "    # Define callbacks for training\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', # Monitor validation loss\n",
    "                                   patience=EARLY_STOPPING_PATIENCE,\n",
    "                                   restore_best_weights=True, # Keep the best model weights\n",
    "                                   verbose=1)\n",
    "\n",
    "    # Train the model\n",
    "    print(\"\\nTraining the model...\")\n",
    "    try:\n",
    "      # Ensure y_train is in the correct format (categorical)\n",
    "      if y_train.shape[1] != N_CLASSES:\n",
    "           raise ValueError(f\"y_train shape {y_train.shape} is not compatible with N_CLASSES={N_CLASSES}\")\n",
    "\n",
    "      history = model.fit(X_train_processed, y_train,\n",
    "                          epochs=EPOCHS,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          validation_split=VALIDATION_SPLIT, # Use part of training data for validation\n",
    "                          callbacks=[early_stopping],\n",
    "                          verbose=1) # Show training progress\n",
    "      print(\"\\nModel training finished.\")\n",
    "\n",
    "      # Plot training history (Loss and Accuracy)\n",
    "      if history is not None:\n",
    "          pd.DataFrame(history.history).plot(figsize=(10, 6))\n",
    "          plt.grid(True)\n",
    "          plt.title('Model Training History')\n",
    "          plt.xlabel('Epoch')\n",
    "          plt.ylabel('Metric Value')\n",
    "          # Dynamically adjust legend based on available history keys\n",
    "          legend_items = [key for key in ['loss', 'accuracy', 'val_loss', 'val_accuracy'] if key in history.history]\n",
    "          plt.legend(legend_items)\n",
    "          plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR during model training: {e}\")\n",
    "        traceback.print_exc() # Print detailed traceback\n",
    "        model = None # Ensure model is None if training failed\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping model building and training due to error or lack of data during preprocessing steps.\")\n",
    "\n",
    "\n",
    "# --- Evaluate Model ---\n",
    "# Check if model exists and test data is valid\n",
    "if model is not None and X_test_processed is not None and y_test is not None and preprocessing_info is not None:\n",
    "    print(\"\\nEvaluating model on the test set...\")\n",
    "    try:\n",
    "        # Ensure y_test is valid before evaluation\n",
    "        if y_test.shape[1] != N_CLASSES:\n",
    "            raise ValueError(f\"y_test shape {y_test.shape} is not compatible with N_CLASSES={N_CLASSES}\")\n",
    "\n",
    "        loss, accuracy = model.evaluate(X_test_processed, y_test, verbose=0)\n",
    "        print(f\"Test Loss: {loss:.4f}\")\n",
    "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # Get predictions (probabilities and predicted class index)\n",
    "        y_pred_proba = model.predict(X_test_processed)\n",
    "        y_pred_classes = np.argmax(y_pred_proba, axis=1) # Index of the max probability class\n",
    "        y_test_classes = np.argmax(y_test, axis=1) # Convert one-hot encoded test labels back to class indices\n",
    "\n",
    "        # Map predicted classes back to original signals (-1, 0, 1) for interpretation\n",
    "        predicted_signals = np.array([preprocessing_info['signal_mapping_inv'].get(p, 0) for p in y_pred_classes]) # Use .get for safety\n",
    "        actual_signals_test = np.array([preprocessing_info['signal_mapping_inv'].get(p, 0) for p in y_test_classes])\n",
    "\n",
    "        # Classification Report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        # Use actual signals (-1, 0, 1) for report labels\n",
    "        print(classification_report(actual_signals_test, predicted_signals, labels=[-1, 0, 1], target_names=['Short (-1)', 'Hold (0)', 'Long (1)'], zero_division=0))\n",
    "\n",
    "        # Confusion Matrix\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        cm = confusion_matrix(actual_signals_test, predicted_signals, labels=[-1, 0, 1]) # Ensure labels are ordered\n",
    "        print(pd.DataFrame(cm, index=['Actual Short', 'Actual Hold', 'Actual Long'], columns=['Pred Short', 'Pred Hold', 'Pred Long']))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during model evaluation: {e}\")\n",
    "        traceback.print_exc()\n",
    "        predicted_signals = None # Ensure this is None if evaluation fails\n",
    "\n",
    "\n",
    "else:\n",
    "    # Provide more context on why evaluation is skipped\n",
    "    if model is None:\n",
    "        print(\"\\nSkipping model evaluation because the model was not trained successfully.\")\n",
    "    else:\n",
    "        print(\"\\nSkipping model evaluation due to lack of processed test data.\")\n",
    "\n",
    "\n",
    "# --- Backtesting ---\n",
    "# Check if predictions are available and necessary info exists\n",
    "if predicted_signals is not None and preprocessing_info is not None:\n",
    "    print(\"\\nStarting backtest simulation...\")\n",
    "    try:\n",
    "        # Create a DataFrame for backtesting results using the test set dates/indices\n",
    "        backtest_df = pd.DataFrame({\n",
    "            'Actual_Return': preprocessing_info['returns_test'], # The actual return over the 20min window\n",
    "            'Predicted_Signal': predicted_signals,\n",
    "            'Actual_Signal': preprocessing_info['signals_test'] # Actual signal {-1, 0, 1} based on threshold\n",
    "        }, index=preprocessing_info['dates_test'])\n",
    "\n",
    "        # Calculate strategy return based on predicted signal\n",
    "        backtest_df['Strategy_Return'] = 0.0\n",
    "        # Use .loc for assignment to avoid SettingWithCopyWarning\n",
    "        backtest_df.loc[backtest_df['Predicted_Signal'] == 1, 'Strategy_Return'] = backtest_df.loc[backtest_df['Predicted_Signal'] == 1, 'Actual_Return']\n",
    "        backtest_df.loc[backtest_df['Predicted_Signal'] == -1, 'Strategy_Return'] = -backtest_df.loc[backtest_df['Predicted_Signal'] == -1, 'Actual_Return']\n",
    "\n",
    "        # Calculate cumulative returns (geometric compounding)\n",
    "        backtest_df['Cumulative_Strategy_Return'] = (1 + backtest_df['Strategy_Return']).cumprod() - 1\n",
    "\n",
    "        print(\"\\nBacktest Simulation Complete.\")\n",
    "\n",
    "        # --- Display Results ---\n",
    "        total_strategy_return = backtest_df['Cumulative_Strategy_Return'].iloc[-1] if not backtest_df.empty else 0\n",
    "        print(f\"\\nTotal Strategy Return (Test Period): {total_strategy_return:.4%}\")\n",
    "\n",
    "        # Plot cumulative returns\n",
    "        if not backtest_df.empty:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            backtest_df['Cumulative_Strategy_Return'].plot(label='Strategy Cumulative Return')\n",
    "            plt.title(f'Backtest Cumulative Returns ({TRADING_WINDOW} min holding)')\n",
    "            plt.xlabel('Date')\n",
    "            plt.ylabel('Cumulative Return')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"No backtesting data to plot.\")\n",
    "\n",
    "        # Yearly Returns Calculation (Geometric)\n",
    "        print(\"\\nYearly Strategy Returns (Test Period):\")\n",
    "        if not backtest_df.empty and isinstance(backtest_df.index, pd.DatetimeIndex) and len(backtest_df.index.year.unique()) > 0 :\n",
    "             yearly_returns_geo = backtest_df['Strategy_Return'].groupby(backtest_df.index.year).apply(lambda x: (1 + x).prod() - 1)\n",
    "             if not yearly_returns_geo.empty:\n",
    "                 print(yearly_returns_geo.map('{:.4%}'.format))\n",
    "             else:\n",
    "                 print(\"Could not calculate yearly returns (possibly insufficient data span).\")\n",
    "        else:\n",
    "            print(\"No data or insufficient time span for yearly returns calculation.\")\n",
    "\n",
    "        # Store results\n",
    "        trade_results = backtest_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during backtesting: {e}\")\n",
    "        traceback.print_exc()\n",
    "        trade_results = None # Ensure this is None if backtesting fails\n",
    "\n",
    "else:\n",
    "    # Provide more context on why backtesting is skipped\n",
    "    if predicted_signals is None:\n",
    "        print(\"\\nSkipping backtesting because model predictions are not available.\")\n",
    "    else:\n",
    "        print(\"\\nSkipping backtesting due to lack of preprocessing information.\")\n",
    "\n",
    "\n",
    "# --- Final Status Check ---\n",
    "print(\"\\n--- Final Status ---\")\n",
    "final_status = \"Process Completed: Model trained, evaluated, and backtested.\"\n",
    "if df is None:\n",
    "    final_status = \"Process Halted: Input data could not be loaded or processed initially.\"\n",
    "elif df_surprise is None:\n",
    "    final_status = \"Process Halted: Not enough valid surprise event data after processing.\"\n",
    "elif X_train_processed is None or y_train is None or preprocessing_info is None:\n",
    "     final_status = \"Process Halted: Error during data preparation, splitting, or preprocessing.\"\n",
    "elif model is None:\n",
    "    final_status = \"Process Halted: Model training did not complete successfully.\"\n",
    "elif predicted_signals is None:\n",
    "    final_status = \"Process Halted: Model evaluation did not complete successfully.\"\n",
    "elif trade_results is None:\n",
    "    final_status = \"Process Halted: Backtesting did not complete successfully.\"\n",
    "\n",
    "print(final_status)\n",
    "if final_status.startswith(\"Process Completed\"):\n",
    "    print(\"Review the plots and printed metrics for performance details.\")\n",
    "else:\n",
    "    print(\"Please review the error messages above to diagnose the issue.\")\n",
    "\n",
    "print(\"--------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
